{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "25ddb919",
   "metadata": {},
   "source": [
    "Dataset Creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "17ad3d8c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "𑀢𑀫𑀺𑀵𑀺𑀮𑁆 𑀉𑀴𑁆𑀴𑀺𑀝𑀼𑀓\n"
     ]
    }
   ],
   "source": [
    "# pip install aksharamukha\n",
    "\n",
    "from aksharamukha import transliterate\n",
    "\n",
    "tamil_text = \"தமிழில் உள்ளிடுக\"\n",
    "tamil_brahmi = transliterate.process('Tamil', 'TamilBrahmi', tamil_text)\n",
    "print(tamil_brahmi)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f00e8cc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import urllib.request\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "file_path = \"Tamil_Articles_Corpus.txt\"\n",
    "\n",
    "with open(file_path, \"r\", encoding=\"utf-8\") as file:\n",
    "    text_data_norm = file.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "9aa7acb9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ஒலிம்பிக் போட்டிகள் நடந்த இடங்கள் 1. 1896 - ஏதென்ஸ், கிரீஸ் 2. 1900 - பாரிஸ், பிரான்ஸ் 3. 1904 - செய\n"
     ]
    }
   ],
   "source": [
    "tamil = text_data_norm[:100]  # Extracting the first 1000 characters for demonstration\n",
    "print(tamil)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8985f7a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The following file has been created: C:/Users/cowar/Downloads/Tamil_Articles_Corpus.txt/Tamil_Articles_Corpus_TamilTamilBrahmi.txt\n"
     ]
    }
   ],
   "source": [
    "# from aksharamukha import transliterate_file\n",
    "# transliterate_file.process(\n",
    "#     'Tamil', 'TamilBrahmi', file_path, 'Tamil_Articles_Corpus_Brahmi.txt'\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f8fecfc",
   "metadata": {},
   "source": [
    "2.2 Tokenizing Text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "042e0e7d",
   "metadata": {},
   "source": [
    "Loading Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "194ecae3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import urllib.request\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "file_path = \"Tamil_Articles_Corpus_TamilTamilBrahmi.txt\"\n",
    "\n",
    "with open(file_path, \"r\", encoding=\"utf-8\") as file:\n",
    "    text_data = file.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ad754069",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Characters: 2079839456\n"
     ]
    }
   ],
   "source": [
    "total_characters = len(text_data)\n",
    "\n",
    "print(\"Characters:\", total_characters)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "39007d45",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample text: 𑀑𑁆𑀮𑀺𑀫𑁆𑀧𑀺𑀓𑁆 𑀧𑁄𑀝𑁆𑀝𑀺𑀓𑀴𑁆 𑀦𑀝𑀦𑁆𑀢 𑀇𑀝𑀗𑁆𑀓𑀴𑁆 1. 1896 - 𑀏𑀢𑁂𑁆𑀷𑁆𑀲𑁆, 𑀓𑀺𑀭𑀻𑀲𑁆 2. 1900 - 𑀧𑀸𑀭𑀺𑀲𑁆, 𑀧𑀺𑀭𑀸𑀷𑁆𑀲𑁆 3. 1904 - 𑀘𑁂𑁆𑀬𑀺𑀷𑁆 𑀮𑀽𑀬𑀺𑀲𑁆, 𑀅𑀫𑁂𑁆𑀭𑀺𑀓𑁆𑀓𑀸 4. 1908 - 𑀮𑀡𑁆𑀝𑀷𑁆,𑀧𑀺𑀭𑀺𑀝𑁆𑀝𑀷𑁆 5. 1912 - 𑀲𑁆𑀝𑁄𑀓𑁆𑀳𑁄𑀫𑁆, 𑀘𑀼𑀯𑀻𑀝𑀷𑁆 6. 1920 - 𑀆𑀡𑁆𑀝𑁆𑀯𑁂𑁆𑀭𑁆𑀧𑁆, 𑀧𑁂𑁆𑀮𑁆𑀚𑀺𑀬𑀫𑁆 7. 1924 - 𑀧𑀸𑀭𑀺𑀲𑁆, 𑀧𑀺𑀭𑀸𑀷𑁆𑀲𑁆 8. 1928 - 𑀆𑀫𑁆𑀲𑁆𑀝𑀭𑁆𑀝𑀸𑀫𑁆, 𑀳𑀸𑀮𑀦𑁆𑀢𑀼 9. 1932 - 𑀮𑀸𑀲𑁆, 𑀏𑀜𑁆𑀘𑀮𑁆𑀲𑁆 10. 1936 - 𑀧𑁂𑁆𑀭𑁆𑀮𑀺𑀷𑁆, 𑀚𑁂𑁆𑀭𑁆𑀫𑀷𑀺 11. 1948 - 𑀮𑀡𑁆𑀝𑀷𑁆, 𑀇𑀗𑁆𑀓𑀺𑀮𑀸𑀦𑁆𑀢𑀼 12. 1952 - 𑀳𑀮𑁆𑀘𑀺𑀷𑁆𑀓𑀺, 𑀧𑀺𑀷𑁆𑀮𑀸𑀦𑁆𑀢𑀼 13. 1956 - 𑀫𑁂𑀧𑁄𑀭𑁆𑀷𑁆,𑀆𑀲𑁆𑀢𑀺𑀭𑁂𑀮𑀺𑀬𑀸 14. 1960 - 𑀭𑁄𑀫𑁆, 𑀇𑀢𑁆𑀢𑀸𑀮𑀺 15. 1964 - 𑀝𑁄𑀓𑁆𑀓𑀺𑀬𑁄, 𑀚𑀧𑁆𑀧𑀸𑀷𑁆 16. 1968 - 𑀫𑁂𑁆𑀓𑁆𑀘𑀺𑀓𑁄, 𑀫𑁂𑁆𑀓𑁆𑀲𑀺𑀓𑁆𑀓𑁄 17. 1972 - 𑀫𑀺𑀬𑀽𑀷𑀺𑀓𑁆, 𑀚𑁂𑁆𑀭𑁆𑀫𑀷𑀺 18. 1976 - 𑀫𑀸𑀷𑁆𑀝𑁆𑀭𑀺𑀬𑀮𑁆, 𑀓𑀷𑀝𑀸 19. 1980 - 𑀫𑀸𑀲𑁆𑀓𑁄, USSR 20. 1984 - 𑀮𑀸𑀲𑁆 𑀏𑀜𑁆𑀘𑀮𑁆𑀲𑁆, 𑀅𑀫𑁂𑁆𑀭𑀺𑀓𑁆𑀓𑀸 21. 1988 - 𑀘𑀺𑀬𑁄𑀮𑁆, 𑀢𑁂𑁆𑀷𑁆 𑀓𑁄𑁆𑀭𑀺𑀬𑀸 22. 1992 - 𑀧𑀸𑀭𑁆𑀘𑀺𑀮𑁄𑀷𑀸, 𑀲𑁆𑀧𑁂𑁆𑀬𑀺𑀷𑁆 23. 1996 - 𑀅𑀝𑁆𑀮𑀸𑀡𑁆𑀝𑀸, 𑀅𑀫𑁂𑁆𑀭𑀺𑀓𑁆𑀓𑀸 24. 2000 - 𑀘𑀺𑀝𑁆𑀷𑀺, 𑀆𑀲𑁆𑀢𑀺𑀭𑁂𑀮𑀺𑀬𑀸 25. 2004 - 𑀏𑀢𑁂𑁆𑀷𑁆𑀲𑁆, 𑀓𑀺𑀭𑀻𑀲𑁆 26. 2008 - 𑀧𑀻𑀚𑀺𑀗𑁆, 𑀘𑀻𑀷𑀸 27. 2012 - 𑀮𑀡𑁆𑀝𑀷𑁆, 𑀇𑀗𑁆𑀓𑀺𑀮𑀸𑀦𑁆𑀢𑀼 28. 2016 - 𑀭𑀺𑀬𑁄, 𑀧𑀺𑀭𑁂𑀘𑀺𑀮𑁆 29. 2020 - 𑀝𑁄𑀓𑁆𑀓𑀺𑀬𑁄, 𑀚𑀧𑁆𑀧𑀸𑀷𑁆 30. 2024 - 𑀧𑀸𑀭𑀺𑀲𑁆, 𑀧𑀺𑀭𑀸𑀷𑁆𑀲𑁆 31. 2028 - 𑀮𑀸𑀲𑁆 𑀏𑀜𑁆𑀘𑀮𑁆𑀲𑁆, 𑀅𑀫𑁂𑁆𑀭𑀺𑀓𑁆𑀓𑀸 Music ncs: https://www.youtube\n"
     ]
    }
   ],
   "source": [
    "print(\"Sample text:\", text_data[:1000])  # Print first 100 characters for a quick check"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04c77b29",
   "metadata": {},
   "source": [
    "Preprocessor before getting vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8885259",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaned tokens:\n",
      "['<|Next Line|>', 'ஒலிம்பிக்', 'போட்டிகள்', 'நடந்த', 'இடங்கள்', '1', '.', '1896', '-', 'ஏதென்ஸ்', ',', 'கிரீஸ்', '2', '.', '1900', '-', 'பாரிஸ்', ',', 'பிரான்ஸ்', '3', '.', '1904', '-', 'செய', '<|Next Line|>', 'ஒலிம்பிக்', 'போட்டிகள்', 'நடந்த', 'இடங்கள்', '1', '.', '1896', '-', 'ஏதென்ஸ்', ',', 'கிரீஸ்', '2', '.', '1900', '-', 'பாரிஸ்', ',', 'பிரான்ஸ்', '3', '.', '1904', '-', 'செய', '<|Next Line|>']\n",
      "\n",
      "Count of <|Next Line|> in cleaned : 3\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "def preprocess(raw_text):\n",
    "    \"\"\"\n",
    "    Split raw text into tokens, retaining custom <|Next Line|> markers,\n",
    "    non-English tokens, and months/years (number tokens).\n",
    "    \"\"\"\n",
    "\n",
    "    # Remove URLs\n",
    "    raw_text = re.sub(r'https?://\\S+', '', raw_text)\n",
    "    \n",
    "    # Normalize first by adding a marker for new lines\n",
    "    raw_text = raw_text.replace('\\n', ' <|Next Line|> ')\n",
    "\n",
    "    # Split by punctuation or whitespace, but keep <|Next Line|> together\n",
    "    result = re.split(r'(\\<\\|Next Line\\|>|[<>,.:;?_!\"()\\']|--|\\s)', raw_text)\n",
    "\n",
    "    # Filter out empty or whitespace tokens\n",
    "    cleaned = [item for item in result if item.strip()]\n",
    "    \n",
    "    # Filter out English tokens except <|Next Line|> and number tokens\n",
    "    cleaned = [item for item in cleaned\n",
    "               if item == \"<|Next Line|>\" or\n",
    "               item.isdigit() or\n",
    "               not re.search(r'[a-zA-Z]', item)]\n",
    "\n",
    "    return cleaned\n",
    "\n",
    "\n",
    "# 🔹Example usage🔹:\n",
    "\n",
    "raw_text = \"\"\"\n",
    "ஒலிம்பிக் போட்டிகள் நடந்த இடங்கள் 1. 1896 - ஏதென்ஸ், கிரீஸ் 2. 1900 - பாரிஸ், பிரான்ஸ் 3. 1904 - செய\n",
    "ஒலிம்பிக் போட்டிகள் நடந்த இடங்கள் 1. 1896 - ஏதென்ஸ், கிரீஸ் 2. 1900 - பாரிஸ், பிரான்ஸ் 3. 1904 - செய\n",
    "\"\"\"    \n",
    "\n",
    "cleaned = preprocess(raw_text)\n",
    "print(\"Cleaned tokens:\")\n",
    "print(cleaned)\n",
    "print(\"\\nCount of <|Next Line|> in cleaned :\", cleaned.count(\"<|Next Line|>\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d37bde5",
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessed = (preprocess(text_data))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac72650d",
   "metadata": {},
   "source": [
    "2.3 Converting tokens into token IDs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9936b28",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_words = sorted(set(preprocessed))\n",
    "vocab_size = len(all_words)\n",
    "print(f\"Vocabulary size: {vocab_size}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce63e6d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = {word : i for i, word in enumerate(all_words)}\n",
    "print(vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e9c814f",
   "metadata": {},
   "source": [
    "Tokenizer Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5860b5b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleTokenizerV1:\n",
    "    def __init__(self, vocab):\n",
    "        self.str_to_int = vocab\n",
    "        self.int_to_str = {i: word for word, i in vocab.items()}\n",
    "\n",
    "    def encode(self,text):\n",
    "        preprocessed = (preprocess(text))\n",
    "        preprocessed = [item for item in preprocessed if item.strip()]\n",
    "        ids = [self.str_to_int[word] for word in preprocessed]\n",
    "        return ids\n",
    "    \n",
    "    def decode(self,ids):\n",
    "        text = \" \".join([self.int_to_str[i] for i in ids])\n",
    "        text = re.sub(r'\\s+([,.?!\"()\\'])', r'\\1', text)\n",
    "        return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23ddaa6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = SimpleTokenizerV1(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7890ef75",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"\"\"I HAD always thought Jack Gisburn rather a cheap genius--though a good fellow enough--so it was no great surprise to me to hear that, in the height of his glory, he had dropped his painting, married a rich widow, and established himself in a villa on the Riviera. (Though I rather thought it would have been Rome or Florence.)\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dca0f462",
   "metadata": {},
   "outputs": [],
   "source": [
    "ids = tokenizer.encode(text)\n",
    "print(ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb3c29ed",
   "metadata": {},
   "source": [
    "2.4 Adding special context tokens - unknown words and such"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf6b5710",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_tokens = sorted(list(set(preprocessed)))\n",
    "all_tokens.extend([\"<unk>\"])\n",
    "\n",
    "vocab = {word: i for i, word in enumerate(all_tokens)}\n",
    "print(len(vocab))\n",
    "\n",
    "for i, word in enumerate(list(vocab.items())[-5:]):\n",
    "    print(f\"{i}: {word}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75339625",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TamilTokenizerV1:\n",
    "    def __init__(self, vocab):\n",
    "        self.str_to_int = vocab\n",
    "        self.int_to_str = {i: word for word, i in vocab.items()}\n",
    "\n",
    "    def encode(self,text):\n",
    "        preprocessed = (preprocess(text))\n",
    "        preprocessed = [item for item in preprocessed if item.strip()]\n",
    "\n",
    "        # Add <unk> token for unknown words\n",
    "        preprocessed = [item if item in self.str_to_int else \"<unk>\" for item in preprocessed]\n",
    "\n",
    "        ids = [self.str_to_int[word] for word in preprocessed]\n",
    "        return ids\n",
    "    \n",
    "    def decode(self,ids):\n",
    "        text = \" \".join([self.int_to_str[i] for i in ids])\n",
    "        text = re.sub(r'\\s+([,.?!\"()\\'])', r'\\1', text)\n",
    "        return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f89005cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = TamilTokenizerV1(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b31f6bd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.encode(\"I am  helllooo genius.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1066b842",
   "metadata": {},
   "source": [
    "2.5 Byte Pair Encoding (BPE) Ignored"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80a1a4b7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
