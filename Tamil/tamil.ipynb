{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "25ddb919",
   "metadata": {},
   "source": [
    "Dataset Creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "17ad3d8c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğ‘€¢ğ‘€«ğ‘€ºğ‘€µğ‘€ºğ‘€®ğ‘† ğ‘€‰ğ‘€´ğ‘†ğ‘€´ğ‘€ºğ‘€ğ‘€¼ğ‘€“\n"
     ]
    }
   ],
   "source": [
    "# pip install aksharamukha\n",
    "\n",
    "from aksharamukha import transliterate\n",
    "\n",
    "tamil_text = \"à®¤à®®à®¿à®´à®¿à®²à¯ à®‰à®³à¯à®³à®¿à®Ÿà¯à®•\"\n",
    "tamil_brahmi = transliterate.process('Tamil', 'TamilBrahmi', tamil_text)\n",
    "print(tamil_brahmi)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f00e8cc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import urllib.request\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "file_path = \"Tamil_Articles_Corpus.txt\"\n",
    "\n",
    "with open(file_path, \"r\", encoding=\"utf-8\") as file:\n",
    "    text_data_norm = file.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "9aa7acb9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "à®’à®²à®¿à®®à¯à®ªà®¿à®•à¯ à®ªà¯‹à®Ÿà¯à®Ÿà®¿à®•à®³à¯ à®¨à®Ÿà®¨à¯à®¤ à®‡à®Ÿà®™à¯à®•à®³à¯ 1. 1896 - à®à®¤à¯†à®©à¯à®¸à¯, à®•à®¿à®°à¯€à®¸à¯ 2. 1900 - à®ªà®¾à®°à®¿à®¸à¯, à®ªà®¿à®°à®¾à®©à¯à®¸à¯ 3. 1904 - à®šà¯†à®¯\n"
     ]
    }
   ],
   "source": [
    "tamil = text_data_norm[:100]  # Extracting the first 1000 characters for demonstration\n",
    "print(tamil)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8985f7a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The following file has been created: C:/Users/cowar/Downloads/Tamil_Articles_Corpus.txt/Tamil_Articles_Corpus_TamilTamilBrahmi.txt\n"
     ]
    }
   ],
   "source": [
    "# from aksharamukha import transliterate_file\n",
    "# transliterate_file.process(\n",
    "#     'Tamil', 'TamilBrahmi', file_path, 'Tamil_Articles_Corpus_Brahmi.txt'\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f8fecfc",
   "metadata": {},
   "source": [
    "2.2 Tokenizing Text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "042e0e7d",
   "metadata": {},
   "source": [
    "Loading Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "194ecae3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import urllib.request\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "file_path = \"Tamil_Articles_Corpus_TamilTamilBrahmi.txt\"\n",
    "\n",
    "with open(file_path, \"r\", encoding=\"utf-8\") as file:\n",
    "    text_data = file.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ad754069",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Characters: 2079839456\n"
     ]
    }
   ],
   "source": [
    "total_characters = len(text_data)\n",
    "\n",
    "print(\"Characters:\", total_characters)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "39007d45",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample text: ğ‘€‘ğ‘†ğ‘€®ğ‘€ºğ‘€«ğ‘†ğ‘€§ğ‘€ºğ‘€“ğ‘† ğ‘€§ğ‘„ğ‘€ğ‘†ğ‘€ğ‘€ºğ‘€“ğ‘€´ğ‘† ğ‘€¦ğ‘€ğ‘€¦ğ‘†ğ‘€¢ ğ‘€‡ğ‘€ğ‘€—ğ‘†ğ‘€“ğ‘€´ğ‘† 1. 1896 - ğ‘€ğ‘€¢ğ‘‚ğ‘†ğ‘€·ğ‘†ğ‘€²ğ‘†, ğ‘€“ğ‘€ºğ‘€­ğ‘€»ğ‘€²ğ‘† 2. 1900 - ğ‘€§ğ‘€¸ğ‘€­ğ‘€ºğ‘€²ğ‘†, ğ‘€§ğ‘€ºğ‘€­ğ‘€¸ğ‘€·ğ‘†ğ‘€²ğ‘† 3. 1904 - ğ‘€˜ğ‘‚ğ‘†ğ‘€¬ğ‘€ºğ‘€·ğ‘† ğ‘€®ğ‘€½ğ‘€¬ğ‘€ºğ‘€²ğ‘†, ğ‘€…ğ‘€«ğ‘‚ğ‘†ğ‘€­ğ‘€ºğ‘€“ğ‘†ğ‘€“ğ‘€¸ 4. 1908 - ğ‘€®ğ‘€¡ğ‘†ğ‘€ğ‘€·ğ‘†,ğ‘€§ğ‘€ºğ‘€­ğ‘€ºğ‘€ğ‘†ğ‘€ğ‘€·ğ‘† 5. 1912 - ğ‘€²ğ‘†ğ‘€ğ‘„ğ‘€“ğ‘†ğ‘€³ğ‘„ğ‘€«ğ‘†, ğ‘€˜ğ‘€¼ğ‘€¯ğ‘€»ğ‘€ğ‘€·ğ‘† 6. 1920 - ğ‘€†ğ‘€¡ğ‘†ğ‘€ğ‘†ğ‘€¯ğ‘‚ğ‘†ğ‘€­ğ‘†ğ‘€§ğ‘†, ğ‘€§ğ‘‚ğ‘†ğ‘€®ğ‘†ğ‘€šğ‘€ºğ‘€¬ğ‘€«ğ‘† 7. 1924 - ğ‘€§ğ‘€¸ğ‘€­ğ‘€ºğ‘€²ğ‘†, ğ‘€§ğ‘€ºğ‘€­ğ‘€¸ğ‘€·ğ‘†ğ‘€²ğ‘† 8. 1928 - ğ‘€†ğ‘€«ğ‘†ğ‘€²ğ‘†ğ‘€ğ‘€­ğ‘†ğ‘€ğ‘€¸ğ‘€«ğ‘†, ğ‘€³ğ‘€¸ğ‘€®ğ‘€¦ğ‘†ğ‘€¢ğ‘€¼ 9. 1932 - ğ‘€®ğ‘€¸ğ‘€²ğ‘†, ğ‘€ğ‘€œğ‘†ğ‘€˜ğ‘€®ğ‘†ğ‘€²ğ‘† 10. 1936 - ğ‘€§ğ‘‚ğ‘†ğ‘€­ğ‘†ğ‘€®ğ‘€ºğ‘€·ğ‘†, ğ‘€šğ‘‚ğ‘†ğ‘€­ğ‘†ğ‘€«ğ‘€·ğ‘€º 11. 1948 - ğ‘€®ğ‘€¡ğ‘†ğ‘€ğ‘€·ğ‘†, ğ‘€‡ğ‘€—ğ‘†ğ‘€“ğ‘€ºğ‘€®ğ‘€¸ğ‘€¦ğ‘†ğ‘€¢ğ‘€¼ 12. 1952 - ğ‘€³ğ‘€®ğ‘†ğ‘€˜ğ‘€ºğ‘€·ğ‘†ğ‘€“ğ‘€º, ğ‘€§ğ‘€ºğ‘€·ğ‘†ğ‘€®ğ‘€¸ğ‘€¦ğ‘†ğ‘€¢ğ‘€¼ 13. 1956 - ğ‘€«ğ‘‚ğ‘€§ğ‘„ğ‘€­ğ‘†ğ‘€·ğ‘†,ğ‘€†ğ‘€²ğ‘†ğ‘€¢ğ‘€ºğ‘€­ğ‘‚ğ‘€®ğ‘€ºğ‘€¬ğ‘€¸ 14. 1960 - ğ‘€­ğ‘„ğ‘€«ğ‘†, ğ‘€‡ğ‘€¢ğ‘†ğ‘€¢ğ‘€¸ğ‘€®ğ‘€º 15. 1964 - ğ‘€ğ‘„ğ‘€“ğ‘†ğ‘€“ğ‘€ºğ‘€¬ğ‘„, ğ‘€šğ‘€§ğ‘†ğ‘€§ğ‘€¸ğ‘€·ğ‘† 16. 1968 - ğ‘€«ğ‘‚ğ‘†ğ‘€“ğ‘†ğ‘€˜ğ‘€ºğ‘€“ğ‘„, ğ‘€«ğ‘‚ğ‘†ğ‘€“ğ‘†ğ‘€²ğ‘€ºğ‘€“ğ‘†ğ‘€“ğ‘„ 17. 1972 - ğ‘€«ğ‘€ºğ‘€¬ğ‘€½ğ‘€·ğ‘€ºğ‘€“ğ‘†, ğ‘€šğ‘‚ğ‘†ğ‘€­ğ‘†ğ‘€«ğ‘€·ğ‘€º 18. 1976 - ğ‘€«ğ‘€¸ğ‘€·ğ‘†ğ‘€ğ‘†ğ‘€­ğ‘€ºğ‘€¬ğ‘€®ğ‘†, ğ‘€“ğ‘€·ğ‘€ğ‘€¸ 19. 1980 - ğ‘€«ğ‘€¸ğ‘€²ğ‘†ğ‘€“ğ‘„, USSR 20. 1984 - ğ‘€®ğ‘€¸ğ‘€²ğ‘† ğ‘€ğ‘€œğ‘†ğ‘€˜ğ‘€®ğ‘†ğ‘€²ğ‘†, ğ‘€…ğ‘€«ğ‘‚ğ‘†ğ‘€­ğ‘€ºğ‘€“ğ‘†ğ‘€“ğ‘€¸ 21. 1988 - ğ‘€˜ğ‘€ºğ‘€¬ğ‘„ğ‘€®ğ‘†, ğ‘€¢ğ‘‚ğ‘†ğ‘€·ğ‘† ğ‘€“ğ‘„ğ‘†ğ‘€­ğ‘€ºğ‘€¬ğ‘€¸ 22. 1992 - ğ‘€§ğ‘€¸ğ‘€­ğ‘†ğ‘€˜ğ‘€ºğ‘€®ğ‘„ğ‘€·ğ‘€¸, ğ‘€²ğ‘†ğ‘€§ğ‘‚ğ‘†ğ‘€¬ğ‘€ºğ‘€·ğ‘† 23. 1996 - ğ‘€…ğ‘€ğ‘†ğ‘€®ğ‘€¸ğ‘€¡ğ‘†ğ‘€ğ‘€¸, ğ‘€…ğ‘€«ğ‘‚ğ‘†ğ‘€­ğ‘€ºğ‘€“ğ‘†ğ‘€“ğ‘€¸ 24. 2000 - ğ‘€˜ğ‘€ºğ‘€ğ‘†ğ‘€·ğ‘€º, ğ‘€†ğ‘€²ğ‘†ğ‘€¢ğ‘€ºğ‘€­ğ‘‚ğ‘€®ğ‘€ºğ‘€¬ğ‘€¸ 25. 2004 - ğ‘€ğ‘€¢ğ‘‚ğ‘†ğ‘€·ğ‘†ğ‘€²ğ‘†, ğ‘€“ğ‘€ºğ‘€­ğ‘€»ğ‘€²ğ‘† 26. 2008 - ğ‘€§ğ‘€»ğ‘€šğ‘€ºğ‘€—ğ‘†, ğ‘€˜ğ‘€»ğ‘€·ğ‘€¸ 27. 2012 - ğ‘€®ğ‘€¡ğ‘†ğ‘€ğ‘€·ğ‘†, ğ‘€‡ğ‘€—ğ‘†ğ‘€“ğ‘€ºğ‘€®ğ‘€¸ğ‘€¦ğ‘†ğ‘€¢ğ‘€¼ 28. 2016 - ğ‘€­ğ‘€ºğ‘€¬ğ‘„, ğ‘€§ğ‘€ºğ‘€­ğ‘‚ğ‘€˜ğ‘€ºğ‘€®ğ‘† 29. 2020 - ğ‘€ğ‘„ğ‘€“ğ‘†ğ‘€“ğ‘€ºğ‘€¬ğ‘„, ğ‘€šğ‘€§ğ‘†ğ‘€§ğ‘€¸ğ‘€·ğ‘† 30. 2024 - ğ‘€§ğ‘€¸ğ‘€­ğ‘€ºğ‘€²ğ‘†, ğ‘€§ğ‘€ºğ‘€­ğ‘€¸ğ‘€·ğ‘†ğ‘€²ğ‘† 31. 2028 - ğ‘€®ğ‘€¸ğ‘€²ğ‘† ğ‘€ğ‘€œğ‘†ğ‘€˜ğ‘€®ğ‘†ğ‘€²ğ‘†, ğ‘€…ğ‘€«ğ‘‚ğ‘†ğ‘€­ğ‘€ºğ‘€“ğ‘†ğ‘€“ğ‘€¸ Music ncs: https://www.youtube\n"
     ]
    }
   ],
   "source": [
    "print(\"Sample text:\", text_data[:1000])  # Print first 100 characters for a quick check"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04c77b29",
   "metadata": {},
   "source": [
    "Preprocessor before getting vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8885259",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaned tokens:\n",
      "['<|Next Line|>', 'à®’à®²à®¿à®®à¯à®ªà®¿à®•à¯', 'à®ªà¯‹à®Ÿà¯à®Ÿà®¿à®•à®³à¯', 'à®¨à®Ÿà®¨à¯à®¤', 'à®‡à®Ÿà®™à¯à®•à®³à¯', '1', '.', '1896', '-', 'à®à®¤à¯†à®©à¯à®¸à¯', ',', 'à®•à®¿à®°à¯€à®¸à¯', '2', '.', '1900', '-', 'à®ªà®¾à®°à®¿à®¸à¯', ',', 'à®ªà®¿à®°à®¾à®©à¯à®¸à¯', '3', '.', '1904', '-', 'à®šà¯†à®¯', '<|Next Line|>', 'à®’à®²à®¿à®®à¯à®ªà®¿à®•à¯', 'à®ªà¯‹à®Ÿà¯à®Ÿà®¿à®•à®³à¯', 'à®¨à®Ÿà®¨à¯à®¤', 'à®‡à®Ÿà®™à¯à®•à®³à¯', '1', '.', '1896', '-', 'à®à®¤à¯†à®©à¯à®¸à¯', ',', 'à®•à®¿à®°à¯€à®¸à¯', '2', '.', '1900', '-', 'à®ªà®¾à®°à®¿à®¸à¯', ',', 'à®ªà®¿à®°à®¾à®©à¯à®¸à¯', '3', '.', '1904', '-', 'à®šà¯†à®¯', '<|Next Line|>']\n",
      "\n",
      "Count of <|Next Line|> in cleaned : 3\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "def preprocess(raw_text):\n",
    "    \"\"\"\n",
    "    Split raw text into tokens, retaining custom <|Next Line|> markers,\n",
    "    non-English tokens, and months/years (number tokens).\n",
    "    \"\"\"\n",
    "\n",
    "    # Remove URLs\n",
    "    raw_text = re.sub(r'https?://\\S+', '', raw_text)\n",
    "    \n",
    "    # Normalize first by adding a marker for new lines\n",
    "    raw_text = raw_text.replace('\\n', ' <|Next Line|> ')\n",
    "\n",
    "    # Split by punctuation or whitespace, but keep <|Next Line|> together\n",
    "    result = re.split(r'(\\<\\|Next Line\\|>|[<>,.:;?_!\"()\\']|--|\\s)', raw_text)\n",
    "\n",
    "    # Filter out empty or whitespace tokens\n",
    "    cleaned = [item for item in result if item.strip()]\n",
    "    \n",
    "    # Filter out English tokens except <|Next Line|> and number tokens\n",
    "    cleaned = [item for item in cleaned\n",
    "               if item == \"<|Next Line|>\" or\n",
    "               item.isdigit() or\n",
    "               not re.search(r'[a-zA-Z]', item)]\n",
    "\n",
    "    return cleaned\n",
    "\n",
    "\n",
    "# ğŸ”¹Example usageğŸ”¹:\n",
    "\n",
    "raw_text = \"\"\"\n",
    "à®’à®²à®¿à®®à¯à®ªà®¿à®•à¯ à®ªà¯‹à®Ÿà¯à®Ÿà®¿à®•à®³à¯ à®¨à®Ÿà®¨à¯à®¤ à®‡à®Ÿà®™à¯à®•à®³à¯ 1. 1896 - à®à®¤à¯†à®©à¯à®¸à¯, à®•à®¿à®°à¯€à®¸à¯ 2. 1900 - à®ªà®¾à®°à®¿à®¸à¯, à®ªà®¿à®°à®¾à®©à¯à®¸à¯ 3. 1904 - à®šà¯†à®¯\n",
    "à®’à®²à®¿à®®à¯à®ªà®¿à®•à¯ à®ªà¯‹à®Ÿà¯à®Ÿà®¿à®•à®³à¯ à®¨à®Ÿà®¨à¯à®¤ à®‡à®Ÿà®™à¯à®•à®³à¯ 1. 1896 - à®à®¤à¯†à®©à¯à®¸à¯, à®•à®¿à®°à¯€à®¸à¯ 2. 1900 - à®ªà®¾à®°à®¿à®¸à¯, à®ªà®¿à®°à®¾à®©à¯à®¸à¯ 3. 1904 - à®šà¯†à®¯\n",
    "\"\"\"    \n",
    "\n",
    "cleaned = preprocess(raw_text)\n",
    "print(\"Cleaned tokens:\")\n",
    "print(cleaned)\n",
    "print(\"\\nCount of <|Next Line|> in cleaned :\", cleaned.count(\"<|Next Line|>\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d37bde5",
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessed = (preprocess(text_data))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac72650d",
   "metadata": {},
   "source": [
    "2.3 Converting tokens into token IDs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9936b28",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_words = sorted(set(preprocessed))\n",
    "vocab_size = len(all_words)\n",
    "print(f\"Vocabulary size: {vocab_size}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce63e6d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = {word : i for i, word in enumerate(all_words)}\n",
    "print(vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e9c814f",
   "metadata": {},
   "source": [
    "Tokenizer Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5860b5b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleTokenizerV1:\n",
    "    def __init__(self, vocab):\n",
    "        self.str_to_int = vocab\n",
    "        self.int_to_str = {i: word for word, i in vocab.items()}\n",
    "\n",
    "    def encode(self,text):\n",
    "        preprocessed = (preprocess(text))\n",
    "        preprocessed = [item for item in preprocessed if item.strip()]\n",
    "        ids = [self.str_to_int[word] for word in preprocessed]\n",
    "        return ids\n",
    "    \n",
    "    def decode(self,ids):\n",
    "        text = \" \".join([self.int_to_str[i] for i in ids])\n",
    "        text = re.sub(r'\\s+([,.?!\"()\\'])', r'\\1', text)\n",
    "        return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23ddaa6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = SimpleTokenizerV1(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7890ef75",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"\"\"I HAD always thought Jack Gisburn rather a cheap genius--though a good fellow enough--so it was no great surprise to me to hear that, in the height of his glory, he had dropped his painting, married a rich widow, and established himself in a villa on the Riviera. (Though I rather thought it would have been Rome or Florence.)\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dca0f462",
   "metadata": {},
   "outputs": [],
   "source": [
    "ids = tokenizer.encode(text)\n",
    "print(ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb3c29ed",
   "metadata": {},
   "source": [
    "2.4 Adding special context tokens - unknown words and such"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf6b5710",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_tokens = sorted(list(set(preprocessed)))\n",
    "all_tokens.extend([\"<unk>\"])\n",
    "\n",
    "vocab = {word: i for i, word in enumerate(all_tokens)}\n",
    "print(len(vocab))\n",
    "\n",
    "for i, word in enumerate(list(vocab.items())[-5:]):\n",
    "    print(f\"{i}: {word}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75339625",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TamilTokenizerV1:\n",
    "    def __init__(self, vocab):\n",
    "        self.str_to_int = vocab\n",
    "        self.int_to_str = {i: word for word, i in vocab.items()}\n",
    "\n",
    "    def encode(self,text):\n",
    "        preprocessed = (preprocess(text))\n",
    "        preprocessed = [item for item in preprocessed if item.strip()]\n",
    "\n",
    "        # Add <unk> token for unknown words\n",
    "        preprocessed = [item if item in self.str_to_int else \"<unk>\" for item in preprocessed]\n",
    "\n",
    "        ids = [self.str_to_int[word] for word in preprocessed]\n",
    "        return ids\n",
    "    \n",
    "    def decode(self,ids):\n",
    "        text = \" \".join([self.int_to_str[i] for i in ids])\n",
    "        text = re.sub(r'\\s+([,.?!\"()\\'])', r'\\1', text)\n",
    "        return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f89005cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = TamilTokenizerV1(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b31f6bd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.encode(\"I am  helllooo genius.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1066b842",
   "metadata": {},
   "source": [
    "2.5 Byte Pair Encoding (BPE) Ignored"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80a1a4b7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
