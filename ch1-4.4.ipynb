{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a1071e72",
   "metadata": {},
   "source": [
    "2.2 Tokenizing Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a3e04c36",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import urllib.request\n",
    "\n",
    "if not os.path.exists(\"the-verdict.txt\"):\n",
    "    url = \"https://raw.githubusercontent.com/rasbt/LLMs-from-scratch/refs/heads/main/ch02/01_main-chapter-code/the-verdict.txt\"\n",
    "    urllib.request.urlretrieve(url, \"the-verdict.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "00cae6e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"the-verdict.txt\", \"r\") as f:\n",
    "    raw_text = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1ecdae58",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20479"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(raw_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f73a1bfc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hello,', ' ', 'world!', ' ', 'This', ' ', 'is', ' ', 'a', ' ', 'test.', ' ', \"Let's\", ' ', 'see', ' ', 'how', ' ', 'it', ' ', 'works.']\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "text = \"Hello, world! This is a test. Let's see how it works.\"\n",
    "\n",
    "result = re.split(r'(\\s)',text)\n",
    "\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "767e0d78",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hello', ',', '', ' ', 'world!', ' ', 'This', ' ', 'is', ' ', 'a', ' ', 'test', '.', '', ' ', \"Let's\", ' ', 'see', ' ', 'how', ' ', 'it', ' ', 'works', '.', '']\n"
     ]
    }
   ],
   "source": [
    "result = re.split(r'([,.]|\\s)',text)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bf8edc68",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hello', ',', 'world!', 'This', 'is', 'a', 'test', '.', \"Let's\", 'see', 'how', 'it', 'works', '.']\n"
     ]
    }
   ],
   "source": [
    "result = [item for item in result if item.strip()]\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b8f01ef3",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"Hello, world! This --- is a test. Let's see how it works.\"\n",
    "\n",
    "result = re.split(r'([,.:;?_!\"()\\']|--|\\s)',raw_text)\n",
    "result = [item for item in result if item.strip()]\n",
    "preprocessed = result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "25955dc7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4690"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(preprocessed)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2ba04a9",
   "metadata": {},
   "source": [
    "2.3 Converting tokens into token IDs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5c0ac7ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size: 1130\n"
     ]
    }
   ],
   "source": [
    "all_words = sorted(set(preprocessed))\n",
    "vocab_size = len(all_words)\n",
    "print(f\"Vocabulary size: {vocab_size}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c1ffe306",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'!': 0, '\"': 1, \"'\": 2, '(': 3, ')': 4, ',': 5, '--': 6, '.': 7, ':': 8, ';': 9, '?': 10, 'A': 11, 'Ah': 12, 'Among': 13, 'And': 14, 'Are': 15, 'Arrt': 16, 'As': 17, 'At': 18, 'Be': 19, 'Begin': 20, 'Burlington': 21, 'But': 22, 'By': 23, 'Carlo': 24, 'Chicago': 25, 'Claude': 26, 'Come': 27, 'Croft': 28, 'Destroyed': 29, 'Devonshire': 30, 'Don': 31, 'Dubarry': 32, 'Emperors': 33, 'Florence': 34, 'For': 35, 'Gallery': 36, 'Gideon': 37, 'Gisburn': 38, 'Gisburns': 39, 'Grafton': 40, 'Greek': 41, 'Grindle': 42, 'Grindles': 43, 'HAD': 44, 'Had': 45, 'Hang': 46, 'Has': 47, 'He': 48, 'Her': 49, 'Hermia': 50, 'His': 51, 'How': 52, 'I': 53, 'If': 54, 'In': 55, 'It': 56, 'Jack': 57, 'Jove': 58, 'Just': 59, 'Lord': 60, 'Made': 61, 'Miss': 62, 'Money': 63, 'Monte': 64, 'Moon-dancers': 65, 'Mr': 66, 'Mrs': 67, 'My': 68, 'Never': 69, 'No': 70, 'Now': 71, 'Nutley': 72, 'Of': 73, 'Oh': 74, 'On': 75, 'Once': 76, 'Only': 77, 'Or': 78, 'Perhaps': 79, 'Poor': 80, 'Professional': 81, 'Renaissance': 82, 'Rickham': 83, 'Riviera': 84, 'Rome': 85, 'Russian': 86, 'Sevres': 87, 'She': 88, 'Stroud': 89, 'Strouds': 90, 'Suddenly': 91, 'That': 92, 'The': 93, 'Then': 94, 'There': 95, 'They': 96, 'This': 97, 'Those': 98, 'Though': 99, 'Thwing': 100, 'Thwings': 101, 'To': 102, 'Usually': 103, 'Venetian': 104, 'Victor': 105, 'Was': 106, 'We': 107, 'Well': 108, 'What': 109, 'When': 110, 'Why': 111, 'Yes': 112, 'You': 113, '_': 114, 'a': 115, 'abdication': 116, 'able': 117, 'about': 118, 'above': 119, 'abruptly': 120, 'absolute': 121, 'absorbed': 122, 'absurdity': 123, 'academic': 124, 'accuse': 125, 'accustomed': 126, 'across': 127, 'activity': 128, 'add': 129, 'added': 130, 'admirers': 131, 'adopted': 132, 'adulation': 133, 'advance': 134, 'aesthetic': 135, 'affect': 136, 'afraid': 137, 'after': 138, 'afterward': 139, 'again': 140, 'ago': 141, 'ah': 142, 'air': 143, 'alive': 144, 'all': 145, 'almost': 146, 'alone': 147, 'along': 148, 'always': 149, 'am': 150, 'amazement': 151, 'amid': 152, 'among': 153, 'amplest': 154, 'amusing': 155, 'an': 156, 'and': 157, 'another': 158, 'answer': 159, 'answered': 160, 'any': 161, 'anything': 162, 'anywhere': 163, 'apparent': 164, 'apparently': 165, 'appearance': 166, 'appeared': 167, 'appointed': 168, 'are': 169, 'arm': 170, 'arm-chair': 171, 'arm-chairs': 172, 'arms': 173, 'art': 174, 'articles': 175, 'artist': 176, 'as': 177, 'aside': 178, 'asked': 179, 'at': 180, 'atmosphere': 181, 'atom': 182, 'attack': 183, 'attention': 184, 'attitude': 185, 'audacities': 186, 'away': 187, 'awful': 188, 'axioms': 189, 'azaleas': 190, 'back': 191, 'background': 192, 'balance': 193, 'balancing': 194, 'balustraded': 195, 'basking': 196, 'bath-rooms': 197, 'be': 198, 'beaming': 199, 'bean-stalk': 200, 'bear': 201, 'beard': 202, 'beauty': 203, 'became': 204, 'because': 205, 'becoming': 206, 'bed': 207, 'been': 208, 'before': 209, 'began': 210, 'begun': 211, 'behind': 212, 'being': 213, 'believed': 214, 'beneath': 215, 'bespoke': 216, 'better': 217, 'between': 218, 'big': 219, 'bits': 220, 'bitterness': 221, 'blocked': 222, 'born': 223, 'borne': 224, 'boudoir': 225, 'bravura': 226, 'break': 227, 'breaking': 228, 'breathing': 229, 'bric-a-brac': 230, 'briefly': 231, 'brings': 232, 'bronzes': 233, 'brought': 234, 'brown': 235, 'brush': 236, 'bull': 237, 'business': 238, 'but': 239, 'buying': 240, 'by': 241, 'called': 242, 'came': 243, 'can': 244, 'canvas': 245, 'canvases': 246, 'cards': 247, 'care': 248, 'career': 249, 'caught': 250, 'central': 251, 'chair': 252, 'chap': 253, 'characteristic': 254, 'charming': 255, 'cheap': 256, 'check': 257, 'cheeks': 258, 'chest': 259, 'chimney-piece': 260, 'chucked': 261, 'cigar': 262, 'cigarette': 263, 'cigars': 264, 'circulation': 265, 'circumstance': 266, 'circus-clown': 267, 'claimed': 268, 'clasping': 269, 'clear': 270, 'cleverer': 271, 'close': 272, 'clue': 273, 'coat': 274, 'collapsed': 275, 'colour': 276, 'come': 277, 'comfortable': 278, 'coming': 279, 'companion': 280, 'compared': 281, 'complex': 282, 'confident': 283, 'congesting': 284, 'conjugal': 285, 'constraint': 286, 'consummate': 287, 'contended': 288, 'continued': 289, 'corner': 290, 'corrected': 291, 'could': 292, 'couldn': 293, 'count': 294, 'countenance': 295, 'couple': 296, 'course': 297, 'covered': 298, 'craft': 299, 'cried': 300, 'crossed': 301, 'crowned': 302, 'crumbled': 303, 'cry': 304, 'cured': 305, 'curiosity': 306, 'curious': 307, 'current': 308, 'curtains': 309, 'd': 310, 'dabble': 311, 'damask': 312, 'dark': 313, 'dashed': 314, 'day': 315, 'days': 316, 'dead': 317, 'deadening': 318, 'dear': 319, 'deep': 320, 'deerhound': 321, 'degree': 322, 'delicate': 323, 'demand': 324, 'denied': 325, 'deploring': 326, 'deprecating': 327, 'deprecatingly': 328, 'desire': 329, 'destroyed': 330, 'destruction': 331, 'desultory': 332, 'detail': 333, 'diagnosis': 334, 'did': 335, 'didn': 336, 'died': 337, 'dim': 338, 'dimmest': 339, 'dingy': 340, 'dining-room': 341, 'disarming': 342, 'discovery': 343, 'discrimination': 344, 'discussion': 345, 'disdain': 346, 'disdained': 347, 'disease': 348, 'disguised': 349, 'display': 350, 'dissatisfied': 351, 'distinguished': 352, 'distract': 353, 'divert': 354, 'do': 355, 'doesn': 356, 'doing': 357, 'domestic': 358, 'don': 359, 'done': 360, 'donkey': 361, 'down': 362, 'dozen': 363, 'dragged': 364, 'drawing-room': 365, 'drawing-rooms': 366, 'drawn': 367, 'dress-closets': 368, 'drew': 369, 'dropped': 370, 'each': 371, 'earth': 372, 'ease': 373, 'easel': 374, 'easy': 375, 'echoed': 376, 'economy': 377, 'effect': 378, 'effects': 379, 'efforts': 380, 'egregious': 381, 'eighteenth-century': 382, 'elbow': 383, 'elegant': 384, 'else': 385, 'embarrassed': 386, 'enabled': 387, 'end': 388, 'endless': 389, 'enjoy': 390, 'enlightenment': 391, 'enough': 392, 'ensuing': 393, 'equally': 394, 'equanimity': 395, 'escape': 396, 'established': 397, 'etching': 398, 'even': 399, 'event': 400, 'ever': 401, 'everlasting': 402, 'every': 403, 'exasperated': 404, 'except': 405, 'excuse': 406, 'excusing': 407, 'existed': 408, 'expected': 409, 'exquisite': 410, 'exquisitely': 411, 'extenuation': 412, 'exterminating': 413, 'extracting': 414, 'eye': 415, 'eyebrows': 416, 'eyes': 417, 'face': 418, 'faces': 419, 'fact': 420, 'faded': 421, 'failed': 422, 'failure': 423, 'fair': 424, 'faith': 425, 'false': 426, 'familiar': 427, 'famille-verte': 428, 'fancy': 429, 'fashionable': 430, 'fate': 431, 'feather': 432, 'feet': 433, 'fell': 434, 'fellow': 435, 'felt': 436, 'few': 437, 'fewer': 438, 'finality': 439, 'find': 440, 'fingers': 441, 'first': 442, 'fit': 443, 'fitting': 444, 'five': 445, 'flash': 446, 'flashed': 447, 'florid': 448, 'flowers': 449, 'fluently': 450, 'flung': 451, 'follow': 452, 'followed': 453, 'fond': 454, 'footstep': 455, 'for': 456, 'forced': 457, 'forcing': 458, 'forehead': 459, 'foreign': 460, 'foreseen': 461, 'forgive': 462, 'forgotten': 463, 'form': 464, 'formed': 465, 'forming': 466, 'forward': 467, 'fostered': 468, 'found': 469, 'foundations': 470, 'fragment': 471, 'fragments': 472, 'frame': 473, 'frames': 474, 'frequently': 475, 'friend': 476, 'from': 477, 'full': 478, 'fullest': 479, 'furiously': 480, 'furrowed': 481, 'garlanded': 482, 'garlands': 483, 'gave': 484, 'genial': 485, 'genius': 486, 'gesture': 487, 'get': 488, 'getting': 489, 'give': 490, 'given': 491, 'glad': 492, 'glanced': 493, 'glimpse': 494, 'gloried': 495, 'glory': 496, 'go': 497, 'going': 498, 'gone': 499, 'good': 500, 'good-breeding': 501, 'good-humoured': 502, 'got': 503, 'grace': 504, 'gradually': 505, 'gray': 506, 'grayish': 507, 'great': 508, 'greatest': 509, 'greatness': 510, 'grew': 511, 'groping': 512, 'growing': 513, 'had': 514, 'hadn': 515, 'hair': 516, 'half': 517, 'half-light': 518, 'half-mechanically': 519, 'hall': 520, 'hand': 521, 'hands': 522, 'handsome': 523, 'hanging': 524, 'happen': 525, 'happened': 526, 'hard': 527, 'hardly': 528, 'has': 529, 'have': 530, 'haven': 531, 'having': 532, 'he': 533, 'head': 534, 'hear': 535, 'heard': 536, 'heart': 537, 'height': 538, 'her': 539, 'here': 540, 'hermit': 541, 'herself': 542, 'hesitations': 543, 'hide': 544, 'high': 545, 'him': 546, 'himself': 547, 'hint': 548, 'his': 549, 'history': 550, 'holding': 551, 'home': 552, 'honour': 553, 'hooded': 554, 'hostess': 555, 'hot-house': 556, 'hour': 557, 'hours': 558, 'house': 559, 'how': 560, 'hung': 561, 'husband': 562, 'idea': 563, 'idle': 564, 'idling': 565, 'if': 566, 'immediately': 567, 'in': 568, 'incense': 569, 'indifferent': 570, 'inevitable': 571, 'inevitably': 572, 'inflexible': 573, 'insensible': 574, 'insignificant': 575, 'instinctively': 576, 'instructive': 577, 'interesting': 578, 'into': 579, 'ironic': 580, 'irony': 581, 'irrelevance': 582, 'irrevocable': 583, 'is': 584, 'it': 585, 'its': 586, 'itself': 587, 'jardiniere': 588, 'jealousy': 589, 'just': 590, 'keep': 591, 'kept': 592, 'kind': 593, 'knees': 594, 'knew': 595, 'know': 596, 'known': 597, 'laid': 598, 'lair': 599, 'landing': 600, 'language': 601, 'last': 602, 'late': 603, 'later': 604, 'latter': 605, 'laugh': 606, 'laughed': 607, 'lay': 608, 'leading': 609, 'lean': 610, 'learned': 611, 'least': 612, 'leathery': 613, 'leave': 614, 'led': 615, 'left': 616, 'leisure': 617, 'lends': 618, 'lent': 619, 'let': 620, 'lies': 621, 'life': 622, 'life-likeness': 623, 'lift': 624, 'lifted': 625, 'light': 626, 'lightly': 627, 'like': 628, 'liked': 629, 'line': 630, 'lines': 631, 'lingered': 632, 'lips': 633, 'lit': 634, 'little': 635, 'live': 636, 'll': 637, 'loathing': 638, 'long': 639, 'longed': 640, 'longer': 641, 'look': 642, 'looked': 643, 'looking': 644, 'lose': 645, 'loss': 646, 'lounging': 647, 'lovely': 648, 'lucky': 649, 'lump': 650, 'luncheon-table': 651, 'luxury': 652, 'lying': 653, 'made': 654, 'make': 655, 'man': 656, 'manage': 657, 'managed': 658, 'mantel-piece': 659, 'marble': 660, 'married': 661, 'may': 662, 'me': 663, 'meant': 664, 'mediocrity': 665, 'medium': 666, 'mentioned': 667, 'mere': 668, 'merely': 669, 'met': 670, 'might': 671, 'mighty': 672, 'millionaire': 673, 'mine': 674, 'minute': 675, 'minutes': 676, 'mirrors': 677, 'modest': 678, 'modesty': 679, 'moment': 680, 'money': 681, 'monumental': 682, 'mood': 683, 'morbidly': 684, 'more': 685, 'most': 686, 'mourn': 687, 'mourned': 688, 'moustache': 689, 'moved': 690, 'much': 691, 'muddling': 692, 'multiplied': 693, 'murmur': 694, 'muscles': 695, 'must': 696, 'my': 697, 'myself': 698, 'mysterious': 699, 'naive': 700, 'near': 701, 'nearly': 702, 'negatived': 703, 'nervous': 704, 'nervousness': 705, 'neutral': 706, 'never': 707, 'next': 708, 'no': 709, 'none': 710, 'not': 711, 'note': 712, 'nothing': 713, 'now': 714, 'nymphs': 715, 'oak': 716, 'obituary': 717, 'object': 718, 'objects': 719, 'occurred': 720, 'oddly': 721, 'of': 722, 'off': 723, 'often': 724, 'oh': 725, 'old': 726, 'on': 727, 'once': 728, 'one': 729, 'ones': 730, 'only': 731, 'onto': 732, 'open': 733, 'or': 734, 'other': 735, 'our': 736, 'ourselves': 737, 'out': 738, 'outline': 739, 'oval': 740, 'over': 741, 'own': 742, 'packed': 743, 'paid': 744, 'paint': 745, 'painted': 746, 'painter': 747, 'painting': 748, 'pale': 749, 'paled': 750, 'palm-trees': 751, 'panel': 752, 'panelling': 753, 'pardonable': 754, 'pardoned': 755, 'part': 756, 'passages': 757, 'passing': 758, 'past': 759, 'pastels': 760, 'pathos': 761, 'patient': 762, 'people': 763, 'perceptible': 764, 'perfect': 765, 'persistence': 766, 'persuasively': 767, 'phrase': 768, 'picture': 769, 'pictures': 770, 'pines': 771, 'pink': 772, 'place': 773, 'placed': 774, 'plain': 775, 'platitudes': 776, 'pleased': 777, 'pockets': 778, 'point': 779, 'poised': 780, 'poor': 781, 'portrait': 782, 'posing': 783, 'possessed': 784, 'poverty': 785, 'predicted': 786, 'preliminary': 787, 'presenting': 788, 'prestidigitation': 789, 'pretty': 790, 'previous': 791, 'price': 792, 'pride': 793, 'princely': 794, 'prism': 795, 'problem': 796, 'proclaiming': 797, 'prodigious': 798, 'profusion': 799, 'protest': 800, 'prove': 801, 'public': 802, 'purblind': 803, 'purely': 804, 'pushed': 805, 'put': 806, 'qualities': 807, 'quality': 808, 'queerly': 809, 'question': 810, 'quickly': 811, 'quietly': 812, 'quite': 813, 'quote': 814, 'rain': 815, 'raised': 816, 'random': 817, 'rather': 818, 're': 819, 'real': 820, 'really': 821, 'reared': 822, 'reason': 823, 'reassurance': 824, 'recovering': 825, 'recreated': 826, 'reflected': 827, 'reflection': 828, 'regrets': 829, 'relatively': 830, 'remained': 831, 'remember': 832, 'reminded': 833, 'repeating': 834, 'represented': 835, 'reproduction': 836, 'resented': 837, 'resolve': 838, 'resources': 839, 'rest': 840, 'rich': 841, 'ridiculous': 842, 'robbed': 843, 'romantic': 844, 'room': 845, 'rose': 846, 'rs': 847, 'rule': 848, 'run': 849, 's': 850, 'said': 851, 'same': 852, 'satisfaction': 853, 'savour': 854, 'saw': 855, 'say': 856, 'saying': 857, 'says': 858, 'scorn': 859, 'scornful': 860, 'secret': 861, 'see': 862, 'seemed': 863, 'seen': 864, 'self-confident': 865, 'send': 866, 'sensation': 867, 'sensitive': 868, 'sent': 869, 'serious': 870, 'set': 871, 'sex': 872, 'shade': 873, 'shaking': 874, 'shall': 875, 'she': 876, 'shirked': 877, 'short': 878, 'should': 879, 'shoulder': 880, 'shoulders': 881, 'show': 882, 'showed': 883, 'showy': 884, 'shrug': 885, 'shrugged': 886, 'sight': 887, 'sign': 888, 'silent': 889, 'silver': 890, 'similar': 891, 'simpleton': 892, 'simplifications': 893, 'simply': 894, 'since': 895, 'single': 896, 'sitter': 897, 'sitters': 898, 'sketch': 899, 'skill': 900, 'slight': 901, 'slightly': 902, 'slowly': 903, 'small': 904, 'smile': 905, 'smiling': 906, 'sneer': 907, 'so': 908, 'solace': 909, 'some': 910, 'somebody': 911, 'something': 912, 'spacious': 913, 'spaniel': 914, 'speaking-tubes': 915, 'speculations': 916, 'spite': 917, 'splash': 918, 'square': 919, 'stairs': 920, 'stammer': 921, 'stand': 922, 'standing': 923, 'started': 924, 'stay': 925, 'still': 926, 'stocked': 927, 'stood': 928, 'stopped': 929, 'stopping': 930, 'straddling': 931, 'straight': 932, 'strain': 933, 'straining': 934, 'strange': 935, 'straw': 936, 'stream': 937, 'stroke': 938, 'strokes': 939, 'strolled': 940, 'strongest': 941, 'strongly': 942, 'struck': 943, 'studio': 944, 'stuff': 945, 'subject': 946, 'substantial': 947, 'suburban': 948, 'such': 949, 'suddenly': 950, 'suffered': 951, 'sugar': 952, 'suggested': 953, 'sunburn': 954, 'sunburnt': 955, 'sunlit': 956, 'superb': 957, 'sure': 958, 'surest': 959, 'surface': 960, 'surprise': 961, 'surprised': 962, 'surrounded': 963, 'suspected': 964, 'sweetly': 965, 'sweetness': 966, 'swelling': 967, 'swept': 968, 'swum': 969, 't': 970, 'table': 971, 'take': 972, 'taken': 973, 'talking': 974, 'tea': 975, 'tears': 976, 'technicalities': 977, 'technique': 978, 'tell': 979, 'tells': 980, 'tempting': 981, 'terra-cotta': 982, 'terrace': 983, 'terraces': 984, 'terribly': 985, 'than': 986, 'that': 987, 'the': 988, 'their': 989, 'them': 990, 'then': 991, 'there': 992, 'therefore': 993, 'they': 994, 'thin': 995, 'thing': 996, 'things': 997, 'think': 998, 'this': 999, 'thither': 1000, 'those': 1001, 'though': 1002, 'thought': 1003, 'three': 1004, 'threshold': 1005, 'threw': 1006, 'through': 1007, 'throwing': 1008, 'tie': 1009, 'till': 1010, 'time': 1011, 'timorously': 1012, 'tinge': 1013, 'tips': 1014, 'tired': 1015, 'to': 1016, 'told': 1017, 'tone': 1018, 'tones': 1019, 'too': 1020, 'took': 1021, 'tottering': 1022, 'touched': 1023, 'toward': 1024, 'trace': 1025, 'trade': 1026, 'transmute': 1027, 'traps': 1028, 'travelled': 1029, 'tribute': 1030, 'tributes': 1031, 'tricks': 1032, 'tried': 1033, 'trouser-presses': 1034, 'true': 1035, 'truth': 1036, 'turned': 1037, 'twenty': 1038, 'twenty-four': 1039, 'twice': 1040, 'twirling': 1041, 'unaccountable': 1042, 'uncertain': 1043, 'under': 1044, 'underlay': 1045, 'underneath': 1046, 'understand': 1047, 'unexpected': 1048, 'untouched': 1049, 'unusual': 1050, 'up': 1051, 'up-stream': 1052, 'upon': 1053, 'upset': 1054, 'upstairs': 1055, 'us': 1056, 'used': 1057, 'usual': 1058, 'value': 1059, 'varnishing': 1060, 'vases': 1061, 've': 1062, 'veins': 1063, 'velveteen': 1064, 'very': 1065, 'villa': 1066, 'vindicated': 1067, 'virtuosity': 1068, 'vista': 1069, 'vocation': 1070, 'voice': 1071, 'wall': 1072, 'wander': 1073, 'want': 1074, 'wanted': 1075, 'wants': 1076, 'was': 1077, 'wasn': 1078, 'watched': 1079, 'watching': 1080, 'water-colour': 1081, 'waves': 1082, 'way': 1083, 'weekly': 1084, 'weeks': 1085, 'welcome': 1086, 'went': 1087, 'were': 1088, 'what': 1089, 'when': 1090, 'whenever': 1091, 'where': 1092, 'which': 1093, 'while': 1094, 'white': 1095, 'white-panelled': 1096, 'who': 1097, 'whole': 1098, 'whom': 1099, 'why': 1100, 'wide': 1101, 'widow': 1102, 'wife': 1103, 'wild': 1104, 'wincing': 1105, 'window-curtains': 1106, 'wish': 1107, 'with': 1108, 'without': 1109, 'wits': 1110, 'woman': 1111, 'women': 1112, 'won': 1113, 'wonder': 1114, 'wondered': 1115, 'word': 1116, 'work': 1117, 'working': 1118, 'worth': 1119, 'would': 1120, 'wouldn': 1121, 'year': 1122, 'years': 1123, 'yellow': 1124, 'yet': 1125, 'you': 1126, 'younger': 1127, 'your': 1128, 'yourself': 1129}\n"
     ]
    }
   ],
   "source": [
    "vocab = {word : i for i, word in enumerate(all_words)}\n",
    "print(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a226427c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleTokenizerV1:\n",
    "    def __init__(self, vocab):\n",
    "        self.str_to_int = vocab\n",
    "        self.int_to_str = {i: word for word, i in vocab.items()}\n",
    "\n",
    "    def encode(self,text):\n",
    "        preprocessed = re.split(r'([,.:;?_!\"()\\']|--|\\s)',text)\n",
    "        preprocessed = [item for item in preprocessed if item.strip()]\n",
    "        ids = [self.str_to_int[word] for word in preprocessed]\n",
    "        return ids\n",
    "    \n",
    "    def decode(self,ids):\n",
    "        text = \" \".join([self.int_to_str[i] for i in ids])\n",
    "        text = re.sub(r'\\s+([,.?!\"()\\'])', r'\\1', text)\n",
    "        return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "fa0cf148",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = SimpleTokenizerV1(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0dd77f04",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"\"\"I HAD always thought Jack Gisburn rather a cheap genius--though a good fellow enough--so it was no great surprise to me to hear that, in the height of his glory, he had dropped his painting, married a rich widow, and established himself in a villa on the Riviera. (Though I rather thought it would have been Rome or Florence.)\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "23bb8c2b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[53, 44, 149, 1003, 57, 38, 818, 115, 256, 486, 6, 1002, 115, 500, 435, 392, 6, 908, 585, 1077, 709, 508, 961, 1016, 663, 1016, 535, 987, 5, 568, 988, 538, 722, 549, 496, 5, 533, 514, 370, 549, 748, 5, 661, 115, 841, 1102, 5, 157, 397, 547, 568, 115, 1066, 727, 988, 84, 7, 3, 99, 53, 818, 1003, 585, 1120, 530, 208, 85, 734, 34, 7, 4]\n"
     ]
    }
   ],
   "source": [
    "ids = tokenizer.encode(text)\n",
    "print(ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a35ba3ec",
   "metadata": {},
   "source": [
    "2.4 Adding special context tokens - unknown words and such"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1115b86b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1132\n",
      "0: ('younger', 1127)\n",
      "1: ('your', 1128)\n",
      "2: ('yourself', 1129)\n",
      "3: ('<|endoftext|>', 1130)\n",
      "4: ('<unk>', 1131)\n"
     ]
    }
   ],
   "source": [
    "all_tokens = sorted(list(set(preprocessed)))\n",
    "all_tokens.extend([\"<|endoftext|>\",\"<unk>\"])\n",
    "\n",
    "\n",
    "vocab = {word: i for i, word in enumerate(all_tokens)}\n",
    "print(len(vocab))\n",
    "\n",
    "for i, word in enumerate(list(vocab.items())[-5:]):\n",
    "    print(f\"{i}: {word}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a8b9bdef",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleTokenizerV1:\n",
    "    def __init__(self, vocab):\n",
    "        self.str_to_int = vocab\n",
    "        self.int_to_str = {i: word for word, i in vocab.items()}\n",
    "\n",
    "    def encode(self,text):\n",
    "        preprocessed = re.split(r'([,.:;?_!\"()\\']|--|\\s)',text)\n",
    "        preprocessed = [item.strip() for item in preprocessed if item.strip()]\n",
    "\n",
    "        # Add <unk> token for unknown words\n",
    "        preprocessed = [item if item in self.str_to_int else \"<unk>\" for item in preprocessed]\n",
    "\n",
    "        ids = [self.str_to_int[word] for word in preprocessed]\n",
    "        return ids\n",
    "    \n",
    "    def decode(self,ids):\n",
    "        text = \" \".join([self.int_to_str[i] for i in ids])\n",
    "        text = re.sub(r'\\s+([,.?!\"()\\'])', r'\\1', text)\n",
    "        return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f5323721",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = SimpleTokenizerV1(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "982259cb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[53, 150, 1131, 486, 7]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.encode(\"I am  helllooo genius.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14f282ea",
   "metadata": {},
   "source": [
    "2.5 Byte Pair Encoding (BPE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e681c953",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tiktoken"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "93c64c14",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = tiktoken.get_encoding(\"gpt2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f4d7fb20",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[40, 716, 220, 5968, 29680, 78, 15632, 13, 922, 284, 766, 345, 13]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.encode(\"I am  helllooo genius. good to see you.\", allowed_special={\"<|endoftext|>\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ba119a4",
   "metadata": {},
   "source": [
    "2.6 Data sampling with a sliding window"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a3444b4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5145\n",
      "[40, 367, 2885, 1464, 1807, 3619, 402, 271, 10899, 2138, 257, 7026, 15632, 438, 2016, 257, 922, 5891, 1576, 438]\n"
     ]
    }
   ],
   "source": [
    "with open(\"the-verdict.txt\", \"r\") as f:\n",
    "    raw_text = f.read()\n",
    "\n",
    "enc_text = tokenizer.encode(raw_text)\n",
    "print(len(enc_text))\n",
    "print(enc_text[:20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "62b0be41",
   "metadata": {},
   "outputs": [],
   "source": [
    "enc_sample = enc_text[50:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "b6559f5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x: [290, 4920, 2241, 287]\n",
      "y:   [4920, 2241, 287, 257]\n"
     ]
    }
   ],
   "source": [
    "context_size = 4  # 4 tokens\n",
    "x = enc_sample[:context_size]\n",
    "y = enc_sample[1:context_size+1]\n",
    "print(f\"x: {x}\")\n",
    "print(f\"y:   {y}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "b1f08fb3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " and ---->  established\n",
      " and established ---->  himself\n",
      " and established himself ---->  in\n",
      " and established himself in ---->  a\n"
     ]
    }
   ],
   "source": [
    "for i in range(1, context_size + 1):\n",
    "    context = enc_sample[:i]\n",
    "    desired = enc_sample[i]\n",
    "    # print([desired])\n",
    "\n",
    "    print(tokenizer.decode(context), \"---->\", tokenizer.decode([desired]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "bd6c2be0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "5fc33ce6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.7.0+cpu'"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d6bad8b",
   "metadata": {},
   "source": [
    "### Use of Dataset\n",
    "\n",
    "The `Dataset` class from `torch.utils.data` is used to represent a dataset in PyTorch. It provides an interface to load and preprocess data efficiently. By subclassing `Dataset`, you can define how to load your data and what each data sample should look like.\n",
    "\n",
    "In this notebook, the `GPTDatasetV1` class is implemented to tokenize text data and create overlapping sequences of input and target tokens using a sliding window approach. This dataset is then used with a `DataLoader` to enable batch processing and efficient data loading during training or evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "be824efe",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "class GPTDatasetV1(Dataset):\n",
    "    def __init__(self, txt, tokenizer, max_length, stride):\n",
    "        self.input_ids = []\n",
    "        self.target_ids = []\n",
    "\n",
    "        # Tokenize the text\n",
    "        token_ids = tokenizer.encode(txt, allowed_special={\"|endoftext|>\"})\n",
    "\n",
    "        # Use a sliding window to chunk the book into overlapping sequences of max_length\n",
    "        for i in range(0, len(token_ids) - max_length, stride):\n",
    "            input_chunk = token_ids[i:i + max_length]\n",
    "            target_chunk = token_ids[i + 1:i + max_length + 1]\n",
    "            self.input_ids.append(torch.tensor(input_chunk))\n",
    "            self.target_ids.append(torch.tensor(target_chunk))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.input_ids)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.input_ids[idx], self.target_ids[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "d71cfb7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataloader_v1(txt, batch_size=2, max_length=256, stride=128, shuffle=True, drop_last=True, num_workers=0):\n",
    "\n",
    "    #initialize the tokenizer\n",
    "    tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
    "\n",
    "    # Create the dataset\n",
    "    dataset = GPTDatasetV1(txt, tokenizer, max_length, stride)\n",
    "\n",
    "    # Create the DataLoader\n",
    "    dataloader = DataLoader(\n",
    "        dataset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=shuffle,\n",
    "        drop_last=drop_last,\n",
    "        num_workers=num_workers\n",
    "    )\n",
    "\n",
    "    return dataloader\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "f460bc77",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"the-verdict.txt\", \"r\") as f:\n",
    "    raw_text = f.read()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "441bdfdd",
   "metadata": {},
   "source": [
    "```markdown\n",
    "### Importance of Stride in Avoiding Overfitting\n",
    "\n",
    "The `stride` parameter in a sliding window approach determines the step size by which the window moves across the tokenized text. It plays a crucial role in balancing the trade-off between data redundancy and overfitting:\n",
    "\n",
    "1. **Reducing Redundancy**:\n",
    "    - A smaller stride creates overlapping sequences, which increases the amount of training data. However, excessive overlap can lead to redundancy, where the model sees almost identical input-target pairs repeatedly. This redundancy can cause the model to memorize patterns instead of generalizing, leading to overfitting.\n",
    "\n",
    "2. **Increasing Diversity**:\n",
    "    - A larger stride reduces overlap between sequences, ensuring that the model is exposed to more diverse input-target pairs. This diversity helps the model learn generalizable patterns rather than memorizing specific sequences.\n",
    "\n",
    "3. **Efficient Use of Data**:\n",
    "    - By carefully choosing the stride, you can maximize the use of available data without introducing excessive redundancy. This is particularly important when working with limited datasets.\n",
    "\n",
    "#### Example:\n",
    "Suppose we have a tokenized text sequence: `[1, 2, 3, 4, 5, 6, 7, 8]`, and we use a sliding window of size `4`.\n",
    "\n",
    "- With a stride of `1`, the generated sequences will be:\n",
    "  - Input: `[1, 2, 3, 4]`, Target: `[2, 3, 4, 5]`\n",
    "  - Input: `[2, 3, 4, 5]`, Target: `[3, 4, 5, 6]`\n",
    "  - Input: `[3, 4, 5, 6]`, Target: `[4, 5, 6, 7]`\n",
    "  - Input: `[4, 5, 6, 7]`, Target: `[5, 6, 7, 8]`\n",
    "\n",
    "- With a stride of `2`, the generated sequences will be:\n",
    "  - Input: `[1, 2, 3, 4]`, Target: `[2, 3, 4, 5]`\n",
    "  - Input: `[3, 4, 5, 6]`, Target: `[4, 5, 6, 7]`\n",
    "  - Input: `[5, 6, 7, 8]`, Target: `[6, 7, 8]`\n",
    "\n",
    "In summary, the stride parameter helps control the balance between data augmentation (through overlapping sequences) and the risk of overfitting. A well-chosen stride ensures that the model learns meaningful patterns while avoiding memorization of repetitive data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "d440404f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input IDs: tensor([[  366, 47914,  1298,   319],\n",
      "        [  373,   262,   530,  7090]])\n",
      "Target IDs: tensor([[47914,  1298,   319,   326],\n",
      "        [  262,   530,  7090,   883]])\n"
     ]
    }
   ],
   "source": [
    "dataloader = create_dataloader_v1(raw_text, batch_size=2, max_length=4, stride=1, shuffle=True, drop_last=True, num_workers=0)\n",
    "\n",
    "data_iter = iter(dataloader)\n",
    "first_batch = next(data_iter)\n",
    "print(f\"Input IDs: {first_batch[0]}\")\n",
    "print(f\"Target IDs: {first_batch[1]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1824c2a",
   "metadata": {},
   "source": [
    "2.7 Creating token embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "67f42171",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_ids = torch.tensor([2,3,5,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "a1add474",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dir(tokenizer) to see its attributes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "9b32addb",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = 6\n",
    "output_dim = 3\n",
    "\n",
    "# torch.manual_seed(123)\n",
    "\n",
    "embedding_layer = torch.nn.Embedding(tokenizer.n_vocab, output_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "695f4323",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([50257, 3])\n"
     ]
    }
   ],
   "source": [
    "print(embedding_layer.weight.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "6563f6ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter containing:\n",
      "tensor([[ 1.3633,  0.2565, -0.0961],\n",
      "        [-0.9221, -0.7746, -0.9668],\n",
      "        [-0.1743,  1.1869,  0.0251],\n",
      "        ...,\n",
      "        [-0.4915,  0.4663,  1.6264],\n",
      "        [ 0.3493, -1.1968, -0.4429],\n",
      "        [-0.4544,  0.1047,  1.6087]], requires_grad=True)\n",
      "tensor([[-0.6692, -0.8622,  0.5965]], grad_fn=<EmbeddingBackward0>)\n"
     ]
    }
   ],
   "source": [
    "print(embedding_layer.weight)\n",
    "print(embedding_layer(torch.tensor([3]))) # weight with index 3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50dc4686",
   "metadata": {},
   "source": [
    "2.8 Encoding Word Positions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "efaaf7f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = tokenizer.n_vocab\n",
    "output_dim = 256\n",
    "\n",
    "token_embedding_layer = torch.nn.Embedding(vocab_size, output_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "8f9bf2db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input IDs: tensor([[   40,   367,  2885,  1464],\n",
      "        [ 1807,  3619,   402,   271],\n",
      "        [10899,  2138,   257,  7026],\n",
      "        [15632,   438,  2016,   257],\n",
      "        [  922,  5891,  1576,   438],\n",
      "        [  568,   340,   373,   645],\n",
      "        [ 1049,  5975,   284,   502],\n",
      "        [  284,  3285,   326,    11]])\n",
      "Target IDs: tensor([[  367,  2885,  1464,  1807],\n",
      "        [ 3619,   402,   271, 10899],\n",
      "        [ 2138,   257,  7026, 15632],\n",
      "        [  438,  2016,   257,   922],\n",
      "        [ 5891,  1576,   438,   568],\n",
      "        [  340,   373,   645,  1049],\n",
      "        [ 5975,   284,   502,   284],\n",
      "        [ 3285,   326,    11,   287]])\n"
     ]
    }
   ],
   "source": [
    "max_length = 4\n",
    "dataloader = create_dataloader_v1(raw_text, batch_size=8, max_length=max_length, stride=max_length, shuffle=False)\n",
    "\n",
    "data_iter = iter(dataloader)\n",
    "inputs, targets = next(data_iter)\n",
    "\n",
    "print(f\"Input IDs: {inputs}\")\n",
    "print(f\"Target IDs: {targets}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebabba64",
   "metadata": {},
   "source": [
    "Add token embeding with positional embeddings to get the actuial input embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "e70872f5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([8, 4, 256])"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token_embeddings = token_embedding_layer(inputs)\n",
    "token_embeddings.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "db5ea9be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4 256\n"
     ]
    }
   ],
   "source": [
    "context_length = max_length\n",
    "print(context_length,output_dim)\n",
    "\n",
    "pos_embedding_layer = torch.nn.Embedding(context_length, output_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "0095f2ae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0, 1, 2, 3])"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.arange(max_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "a1eb8c23",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[ 0.4088,  1.2109,  0.0140,  ..., -1.5870,  0.7703,  0.4062],\n",
       "        [ 1.7653, -0.5633,  0.7863,  ...,  0.1247,  0.9599,  1.2374],\n",
       "        [-1.3675,  1.7316, -0.8095,  ..., -0.0545,  1.6678,  0.2409],\n",
       "        ...,\n",
       "        [ 2.3657,  1.5845,  0.8899,  ..., -0.0824, -0.0478,  1.8299],\n",
       "        [ 1.8002,  2.4408,  0.2107,  ...,  0.7845, -0.2787,  1.8947],\n",
       "        [-0.0785, -1.0014,  0.3345,  ...,  0.0846,  0.4292,  0.0232]],\n",
       "       requires_grad=True)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pos_embedding_layer.weight\n",
    "token_embedding_layer.weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "80503459",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-1.8071,  0.8408,  1.2023,  ..., -0.3775,  0.9030, -0.0372],\n",
       "        [-0.0054,  1.0021, -0.8043,  ..., -0.4835, -2.1447, -0.3478],\n",
       "        [ 0.4897,  1.4632, -0.4386,  ..., -0.1087, -0.4748,  0.3454],\n",
       "        [-0.2053,  2.1852, -0.5965,  ...,  0.6122, -0.5675, -0.5945]],\n",
       "       grad_fn=<EmbeddingBackward0>)"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pos_embedding_layer(torch.arange(max_length))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "68dde5f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 256])\n"
     ]
    }
   ],
   "source": [
    "pos_embeddings = pos_embedding_layer(torch.arange(max_length))\n",
    "print(pos_embeddings.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "d51123cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([8, 4, 256])\n"
     ]
    }
   ],
   "source": [
    "input_embeddings = token_embeddings + pos_embeddings\n",
    "print(input_embeddings.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "39550812",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "inputs = torch.tensor(\n",
    " [[0.43, 0.15, 0.89], # Your (x^1)\n",
    " [0.55, 0.87, 0.66], # journey (x^2)\n",
    " [0.57, 0.85, 0.64], # starts (x^3)\n",
    " [0.22, 0.58, 0.33], # with (x^4)\n",
    " [0.77, 0.25, 0.10], # one (x^5)\n",
    " [0.05, 0.80, 0.55]] # step (x^6)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "41cb47d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.9544, 1.4950, 1.4754, 0.8434, 0.7070, 1.0865])\n"
     ]
    }
   ],
   "source": [
    "query = inputs[1]  # 2nd input token is the query\n",
    "\n",
    "attn_scores_2 = torch.empty(inputs.shape[0])\n",
    "for i, x_i in enumerate(inputs):\n",
    "    attn_scores_2[i] = torch.dot(x_i, query) # dot product (transpose not necessary here since they are 1-dim vectors)\n",
    "\n",
    "print(attn_scores_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "40486dc5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attention weights: tensor([0.1455, 0.2278, 0.2249, 0.1285, 0.1077, 0.1656])\n",
      "Sum: tensor(1.0000)\n"
     ]
    }
   ],
   "source": [
    "attn_weights_2_tmp = attn_scores_2 / attn_scores_2.sum()\n",
    "\n",
    "print(\"Attention weights:\", attn_weights_2_tmp)\n",
    "print(\"Sum:\", attn_weights_2_tmp.sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "7fecae4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attention weights: tensor([0.1385, 0.2379, 0.2333, 0.1240, 0.1082, 0.1581])\n",
      "Sum: tensor(1.)\n"
     ]
    }
   ],
   "source": [
    "def softmax_naive(x):\n",
    "    return torch.exp(x) / torch.exp(x).sum(dim=0)\n",
    "\n",
    "attn_weights_2_naive = softmax_naive(attn_scores_2)\n",
    "\n",
    "print(\"Attention weights:\", attn_weights_2_naive)\n",
    "print(\"Sum:\", attn_weights_2_naive.sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "1cc53aff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attention weights: tensor([0.1385, 0.2379, 0.2333, 0.1240, 0.1082, 0.1581])\n",
      "Sum: tensor(1.)\n"
     ]
    }
   ],
   "source": [
    "attn_weights_2 = torch.softmax(attn_scores_2, dim=0)\n",
    "\n",
    "print(\"Attention weights:\", attn_weights_2)\n",
    "print(\"Sum:\", attn_weights_2.sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "8dec2714",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.4419, 0.6515, 0.5683])\n"
     ]
    }
   ],
   "source": [
    "query = inputs[1] # 2nd input token is the query\n",
    "\n",
    "context_vec_2 = torch.zeros(query.shape)\n",
    "for i,x_i in enumerate(inputs):\n",
    "    context_vec_2 += attn_weights_2[i]*x_i\n",
    "\n",
    "print(context_vec_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d382d55",
   "metadata": {},
   "source": [
    "3.3.2 Computing attention weights for all input tokens\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "3df810d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.9995, 0.9544, 0.9422, 0.4753, 0.4576, 0.6310],\n",
      "        [0.9544, 1.4950, 1.4754, 0.8434, 0.7070, 1.0865],\n",
      "        [0.9422, 1.4754, 1.4570, 0.8296, 0.7154, 1.0605],\n",
      "        [0.4753, 0.8434, 0.8296, 0.4937, 0.3474, 0.6565],\n",
      "        [0.4576, 0.7070, 0.7154, 0.3474, 0.6654, 0.2935],\n",
      "        [0.6310, 1.0865, 1.0605, 0.6565, 0.2935, 0.9450]])\n"
     ]
    }
   ],
   "source": [
    "attn_scores = torch.empty(6, 6)\n",
    "\n",
    "for i, x_i in enumerate(inputs):\n",
    "    for j, x_j in enumerate(inputs):\n",
    "        attn_scores[i, j] = torch.dot(x_i, x_j)\n",
    "\n",
    "print(attn_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "ea8fb5a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.9995, 0.9544, 0.9422, 0.4753, 0.4576, 0.6310],\n",
      "        [0.9544, 1.4950, 1.4754, 0.8434, 0.7070, 1.0865],\n",
      "        [0.9422, 1.4754, 1.4570, 0.8296, 0.7154, 1.0605],\n",
      "        [0.4753, 0.8434, 0.8296, 0.4937, 0.3474, 0.6565],\n",
      "        [0.4576, 0.7070, 0.7154, 0.3474, 0.6654, 0.2935],\n",
      "        [0.6310, 1.0865, 1.0605, 0.6565, 0.2935, 0.9450]])\n"
     ]
    }
   ],
   "source": [
    "attn_scores = inputs @ inputs.T\n",
    "print(attn_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "b868229a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.2098, 0.2006, 0.1981, 0.1242, 0.1220, 0.1452],\n",
      "        [0.1385, 0.2379, 0.2333, 0.1240, 0.1082, 0.1581],\n",
      "        [0.1390, 0.2369, 0.2326, 0.1242, 0.1108, 0.1565],\n",
      "        [0.1435, 0.2074, 0.2046, 0.1462, 0.1263, 0.1720],\n",
      "        [0.1526, 0.1958, 0.1975, 0.1367, 0.1879, 0.1295],\n",
      "        [0.1385, 0.2184, 0.2128, 0.1420, 0.0988, 0.1896]])\n"
     ]
    }
   ],
   "source": [
    "attn_weights = torch.softmax(attn_scores, dim=-1)\n",
    "print(attn_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "e6a92c99",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Row 2 sum: 1.0\n",
      "All row sums: tensor([1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000])\n"
     ]
    }
   ],
   "source": [
    "row_2_sum = sum([0.1385, 0.2379, 0.2333, 0.1240, 0.1082, 0.1581])\n",
    "print(\"Row 2 sum:\", row_2_sum)\n",
    "\n",
    "print(\"All row sums:\", attn_weights.sum(dim=-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "5fcdd459",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.4421, 0.5931, 0.5790],\n",
      "        [0.4419, 0.6515, 0.5683],\n",
      "        [0.4431, 0.6496, 0.5671],\n",
      "        [0.4304, 0.6298, 0.5510],\n",
      "        [0.4671, 0.5910, 0.5266],\n",
      "        [0.4177, 0.6503, 0.5645]])\n"
     ]
    }
   ],
   "source": [
    "all_context_vecs = attn_weights @ inputs\n",
    "print(all_context_vecs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8015fc70",
   "metadata": {},
   "source": [
    "3.4 Implementing self-attention with trainable weights\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5077892",
   "metadata": {},
   "source": [
    "3.4.1 Computing the attention weights step by step\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "d783c526",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_2 = inputs[1]\n",
    "d_in = inputs.shape[1]\n",
    "d_out = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "e1da072d",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(123)\n",
    "\n",
    "W_query = torch.nn.Parameter(torch.rand(d_in, d_out), requires_grad=False)\n",
    "W_key = torch.nn.Parameter(torch.rand(d_in, d_out), requires_grad=False)\n",
    "W_value = torch.nn.Parameter(torch.rand(d_in, d_out), requires_grad=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "6e9e0948",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[0.2961, 0.5166],\n",
       "        [0.2517, 0.6886],\n",
       "        [0.0740, 0.8665]])"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "W_query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "8f8ec84b",
   "metadata": {},
   "outputs": [],
   "source": [
    "query_2 = x_2 @ W_query # Common for all the inputs to calculate q9(2)\n",
    "key_2 = x_2 @ W_key\n",
    "value_2 = x_2 @ W_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "f2bcacad",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.4306, 1.4551])"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "bae70081",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Keys.shape: torch.Size([6, 2])\n",
      "Values.shape: torch.Size([6, 2])\n"
     ]
    }
   ],
   "source": [
    "keys = inputs @ W_key\n",
    "values = inputs @ W_value\n",
    "\n",
    "print(\"Keys.shape:\", keys.shape)\n",
    "print(\"Values.shape:\", values.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c0c1fe5",
   "metadata": {},
   "source": [
    "Calculate attention scores"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "705faba7",
   "metadata": {},
   "source": [
    "First, lets compute the attention score 22:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "e2b3f01a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1.8524)\n"
     ]
    }
   ],
   "source": [
    "keys_2 = keys[1]\n",
    "attn_score_22 = query_2.dot(keys_2)\n",
    "print(attn_score_22)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "903770d8",
   "metadata": {},
   "source": [
    "Again, we can generalize this computation to all attention scores via matrix\n",
    "multiplication:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "1ffe646e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1.2705, 1.8524, 1.8111, 1.0795, 0.5577, 1.5440])\n"
     ]
    }
   ],
   "source": [
    "attn_scores_2 = query_2 @ keys.T\n",
    "print(attn_scores_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "001c93fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attention weights: tensor([0.1500, 0.2264, 0.2199, 0.1311, 0.0906, 0.1820])\n"
     ]
    }
   ],
   "source": [
    "d_k = keys.shape[1]\n",
    "attn_weights_2 = torch.softmax(attn_scores_2 / d_k**0.5, dim=-1)\n",
    "print(\"Attention weights:\", attn_weights_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "483f7f5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.3061, 0.8210])\n"
     ]
    }
   ],
   "source": [
    "context_vec_2 = attn_weights_2 @ values\n",
    "print(context_vec_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a529ad19",
   "metadata": {},
   "source": [
    "3.4.2 Implementing a compact self-attention Python class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "338451c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "class SelfAttention_v1(nn.Module):\n",
    "    def __init__(self, d_in, d_out):\n",
    "        super().__init__()\n",
    "        self.W_query = nn.Parameter(torch.rand(d_in, d_out))\n",
    "        self.W_key = nn.Parameter(torch.rand(d_in, d_out))\n",
    "        self.W_value = nn.Parameter(torch.rand(d_in, d_out))\n",
    "    def forward(self, x):\n",
    "        keys = x @ self.W_key\n",
    "        queries = x @ self.W_query\n",
    "        values = x @ self.W_value\n",
    "        attn_scores = queries @ keys.T # omega\n",
    "        attn_weights = torch.softmax(\n",
    "        attn_scores / keys.shape[-1]**0.5, dim=-1\n",
    "        )\n",
    "        context_vec = attn_weights @ values\n",
    "        return context_vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "42da8c12",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.2996, 0.8053],\n",
      "        [0.3061, 0.8210],\n",
      "        [0.3058, 0.8203],\n",
      "        [0.2948, 0.7939],\n",
      "        [0.2927, 0.7891],\n",
      "        [0.2990, 0.8040]], grad_fn=<MmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(123)\n",
    "sa_v1 = SelfAttention_v1(d_in, d_out)\n",
    "print(sa_v1(inputs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "1edacadb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter containing:\n",
      "tensor([[-0.1668,  0.2270],\n",
      "        [ 0.5000,  0.1317],\n",
      "        [ 0.1934,  0.6825]], requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([-0.3189,  0.2240, -0.3146], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "m = torch.nn.Linear(2, 3)\n",
    "print(m.weight)\n",
    "print(m.bias)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "cca227cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "class SelfAttention_v2(nn.Module):\n",
    "    def __init__(self, d_in, d_out, qkv_bias=False):\n",
    "        super().__init__()\n",
    "        self.W_query = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.W_key = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.W_value = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "    def forward(self, x):\n",
    "        keys = self.W_key(x)\n",
    "        queries = self.W_query(x)\n",
    "        values = self.W_value(x)\n",
    "        attn_scores = queries @ keys.T # omega\n",
    "        attn_weights = torch.softmax(\n",
    "        attn_scores / keys.shape[-1]**0.5, dim=-1\n",
    "        )\n",
    "        context_vec = attn_weights @ values\n",
    "        return context_vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "ba921abc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.5337, -0.1051],\n",
      "        [-0.5323, -0.1080],\n",
      "        [-0.5323, -0.1079],\n",
      "        [-0.5297, -0.1076],\n",
      "        [-0.5311, -0.1066],\n",
      "        [-0.5299, -0.1081]], grad_fn=<MmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(123)\n",
    "sa_v2 = SelfAttention_v2(d_in, d_out)\n",
    "print(sa_v2(inputs))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79816bd8",
   "metadata": {},
   "source": [
    "3.5 Hiding future words with causal attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "21d93a75",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.1717, 0.1762, 0.1761, 0.1555, 0.1627, 0.1579],\n",
      "        [0.1636, 0.1749, 0.1746, 0.1612, 0.1605, 0.1652],\n",
      "        [0.1637, 0.1749, 0.1746, 0.1611, 0.1606, 0.1651],\n",
      "        [0.1636, 0.1704, 0.1702, 0.1652, 0.1632, 0.1674],\n",
      "        [0.1667, 0.1722, 0.1721, 0.1618, 0.1633, 0.1639],\n",
      "        [0.1624, 0.1709, 0.1706, 0.1654, 0.1625, 0.1682]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n"
     ]
    }
   ],
   "source": [
    "queries = sa_v2.W_query(inputs)\n",
    "keys = sa_v2.W_key(inputs)\n",
    "attn_scores = queries @ keys.T\n",
    "attn_weights = torch.softmax(attn_scores / keys.shape[-1]**0.5, dim=-1)\n",
    "print(attn_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "579f290a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 0., 0., 0., 0., 0.],\n",
      "        [1., 1., 0., 0., 0., 0.],\n",
      "        [1., 1., 1., 0., 0., 0.],\n",
      "        [1., 1., 1., 1., 0., 0.],\n",
      "        [1., 1., 1., 1., 1., 0.],\n",
      "        [1., 1., 1., 1., 1., 1.]])\n"
     ]
    }
   ],
   "source": [
    "context_length = attn_scores.shape[0]\n",
    "mask_simple = torch.tril(torch.ones(context_length, context_length))\n",
    "print(mask_simple)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "1c557019",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.1717, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.1636, 0.1749, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.1637, 0.1749, 0.1746, 0.0000, 0.0000, 0.0000],\n",
      "        [0.1636, 0.1704, 0.1702, 0.1652, 0.0000, 0.0000],\n",
      "        [0.1667, 0.1722, 0.1721, 0.1618, 0.1633, 0.0000],\n",
      "        [0.1624, 0.1709, 0.1706, 0.1654, 0.1625, 0.1682]],\n",
      "       grad_fn=<MulBackward0>)\n"
     ]
    }
   ],
   "source": [
    "masked_simple = attn_weights * mask_simple\n",
    "print(masked_simple)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "e2bab7a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.4833, 0.5167, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.3190, 0.3408, 0.3402, 0.0000, 0.0000, 0.0000],\n",
      "        [0.2445, 0.2545, 0.2542, 0.2468, 0.0000, 0.0000],\n",
      "        [0.1994, 0.2060, 0.2058, 0.1935, 0.1953, 0.0000],\n",
      "        [0.1624, 0.1709, 0.1706, 0.1654, 0.1625, 0.1682]],\n",
      "       grad_fn=<DivBackward0>)\n"
     ]
    }
   ],
   "source": [
    "row_sums = masked_simple.sum(dim=-1, keepdim=True)\n",
    "masked_simple = masked_simple / row_sums\n",
    "print(masked_simple)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "1d7e5c8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.3111,   -inf,   -inf,   -inf,   -inf,   -inf],\n",
      "        [0.1655, 0.2602,   -inf,   -inf,   -inf,   -inf],\n",
      "        [0.1667, 0.2602, 0.2577,   -inf,   -inf,   -inf],\n",
      "        [0.0510, 0.1080, 0.1064, 0.0643,   -inf,   -inf],\n",
      "        [0.1415, 0.1875, 0.1863, 0.0987, 0.1121,   -inf],\n",
      "        [0.0476, 0.1192, 0.1171, 0.0731, 0.0477, 0.0966]],\n",
      "       grad_fn=<MaskedFillBackward0>)\n"
     ]
    }
   ],
   "source": [
    "mask = torch.triu(torch.ones(context_length, context_length), diagonal=1)\n",
    "masked = attn_scores.masked_fill(mask.bool(), -torch.inf)\n",
    "print(masked)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "21e7c45b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.4833, 0.5167, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.3190, 0.3408, 0.3402, 0.0000, 0.0000, 0.0000],\n",
      "        [0.2445, 0.2545, 0.2542, 0.2468, 0.0000, 0.0000],\n",
      "        [0.1994, 0.2060, 0.2058, 0.1935, 0.1953, 0.0000],\n",
      "        [0.1624, 0.1709, 0.1706, 0.1654, 0.1625, 0.1682]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n"
     ]
    }
   ],
   "source": [
    "attn_weights = torch.softmax(masked / keys.shape[-1]**0.5, dim=-1)\n",
    "print(attn_weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "671a92a7",
   "metadata": {},
   "source": [
    "3.5.2 Masking additional attention weights with dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "c2197b7f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[2., 2., 0., 2., 2., 0.],\n",
      "        [0., 0., 0., 2., 0., 2.],\n",
      "        [2., 2., 2., 2., 0., 2.],\n",
      "        [0., 2., 2., 0., 0., 2.],\n",
      "        [0., 2., 0., 2., 0., 2.],\n",
      "        [0., 2., 2., 2., 2., 0.]])\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(123)\n",
    "dropout = torch.nn.Dropout(0.5)\n",
    "example = torch.ones(6,6)\n",
    "print(dropout(example))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "b3a2b2c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[2.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.6380, 0.6816, 0.6804, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.5090, 0.5085, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.4120, 0.0000, 0.3869, 0.0000, 0.0000],\n",
      "        [0.0000, 0.3418, 0.3413, 0.3308, 0.3249, 0.0000]],\n",
      "       grad_fn=<MulBackward0>)\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(123)\n",
    "print(dropout(attn_weights))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76707ffd",
   "metadata": {},
   "source": [
    "3.5.3 Implementing a compact causal attention class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "78925fb5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 6, 3])\n"
     ]
    }
   ],
   "source": [
    "batch = torch.stack((inputs, inputs), dim=0)\n",
    "print(batch.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "d83b32c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "class CausalAttention(nn.Module):\n",
    "    def __init__(self, d_in, d_out, context_length, dropout, qkv_bias=False):\n",
    "        super().__init__()\n",
    "        self.W_query = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.W_key = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.W_value = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.register_buffer(\"mask\", torch.triu(torch.ones(context_length, context_length), diagonal=1))\n",
    "\n",
    "    def forward(self, x):\n",
    "        b, num_tokens, d_in = x.shape\n",
    "        keys = self.W_key(x)\n",
    "        queries = self.W_query(x)\n",
    "        values = self.W_value(x)\n",
    "        attn_scores = queries @ keys.transpose(1, 2)\n",
    "        attn_scores.masked_fill_(\n",
    "        self.mask.bool()[:num_tokens, :num_tokens], -torch.inf)\n",
    "        attn_weights = torch.softmax(\n",
    "        attn_scores / keys.shape[-1]**0.5, dim=-1\n",
    "        )\n",
    "        attn_weights = self.dropout(attn_weights)\n",
    "        context_vec = attn_weights @ values\n",
    "        return context_vec\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2428c5a8",
   "metadata": {},
   "source": [
    " Attention Score = Q  K\n",
    "To compute attention scores, we need to perform a dot product between each query and all keys (for each token).\n",
    "\n",
    "That means we want to do:\n",
    "\n",
    "python\n",
    "Copy\n",
    "Edit\n",
    "(B, T, d) @ (B, d, T)  (B, T, T)\n",
    "To do this, we must transpose keys from (B, T, d) to (B, d, T), which is done by:\n",
    "\n",
    "python\n",
    "Copy\n",
    "Edit\n",
    "keys.transpose(1, 2)  # Swaps dimensions 1 and 2 (T  d)\n",
    "So now:\n",
    "\n",
    "python\n",
    "Copy\n",
    "Edit\n",
    "queries @ keys.transpose(1, 2)\n",
    "shape: (B, T, d) @ (B, d, T)  (B, T, T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "35c31430",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 6, 2])\n",
      "tensor([[[-0.4519,  0.2216],\n",
      "         [-0.5874,  0.0058],\n",
      "         [-0.6300, -0.0632],\n",
      "         [-0.5675, -0.0843],\n",
      "         [-0.5526, -0.0981],\n",
      "         [-0.5299, -0.1081]],\n",
      "\n",
      "        [[-0.4519,  0.2216],\n",
      "         [-0.5874,  0.0058],\n",
      "         [-0.6300, -0.0632],\n",
      "         [-0.5675, -0.0843],\n",
      "         [-0.5526, -0.0981],\n",
      "         [-0.5299, -0.1081]]], grad_fn=<UnsafeViewBackward0>)\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(123)\n",
    "context_length = batch.shape[1]\n",
    "ca = CausalAttention(d_in, d_out, context_length, dropout=0.0)\n",
    "context_vecs = ca(batch)\n",
    "print(context_vecs.shape)\n",
    "print(context_vecs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bac04b8",
   "metadata": {},
   "source": [
    "3.6 Extending single-head attention to multi-head attention"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b369f237",
   "metadata": {},
   "source": [
    "3.6.1 Stacking multiple single-head attention layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "c653f590",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[-0.4519,  0.2216,  0.4772,  0.1063],\n",
      "         [-0.5874,  0.0058,  0.5891,  0.3257],\n",
      "         [-0.6300, -0.0632,  0.6202,  0.3860],\n",
      "         [-0.5675, -0.0843,  0.5478,  0.3589],\n",
      "         [-0.5526, -0.0981,  0.5321,  0.3428],\n",
      "         [-0.5299, -0.1081,  0.5077,  0.3493]],\n",
      "\n",
      "        [[-0.4519,  0.2216,  0.4772,  0.1063],\n",
      "         [-0.5874,  0.0058,  0.5891,  0.3257],\n",
      "         [-0.6300, -0.0632,  0.6202,  0.3860],\n",
      "         [-0.5675, -0.0843,  0.5478,  0.3589],\n",
      "         [-0.5526, -0.0981,  0.5321,  0.3428],\n",
      "         [-0.5299, -0.1081,  0.5077,  0.3493]]], grad_fn=<CatBackward0>)\n",
      "context_vecs.shape: torch.Size([2, 6, 4])\n"
     ]
    }
   ],
   "source": [
    "class MultiHeadAttentionWrapper(nn.Module):\n",
    "\n",
    "    def __init__(self, d_in, d_out, context_length, dropout, num_heads, qkv_bias=False):\n",
    "        super().__init__()\n",
    "        self.heads = nn.ModuleList(\n",
    "            [CausalAttention(d_in, d_out, context_length, dropout, qkv_bias) \n",
    "             for _ in range(num_heads)]\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return torch.cat([head(x) for head in self.heads], dim=-1)\n",
    "\n",
    "\n",
    "torch.manual_seed(123)\n",
    "\n",
    "context_length = batch.shape[1] # This is the number of tokens\n",
    "d_in, d_out = 3, 2\n",
    "mha = MultiHeadAttentionWrapper(\n",
    "    d_in, d_out, context_length, 0.0, num_heads=2\n",
    ")\n",
    "\n",
    "context_vecs = mha(batch)\n",
    "\n",
    "print(context_vecs)\n",
    "print(\"context_vecs.shape:\", context_vecs.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6244ca6",
   "metadata": {},
   "source": [
    "3.6.2 Implementing multi-head attention with weight splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "d8569488",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[0.3190, 0.4858],\n",
      "         [0.2943, 0.3897],\n",
      "         [0.2856, 0.3593],\n",
      "         [0.2693, 0.3873],\n",
      "         [0.2639, 0.3928],\n",
      "         [0.2575, 0.4028]],\n",
      "\n",
      "        [[0.3190, 0.4858],\n",
      "         [0.2943, 0.3897],\n",
      "         [0.2856, 0.3593],\n",
      "         [0.2693, 0.3873],\n",
      "         [0.2639, 0.3928],\n",
      "         [0.2575, 0.4028]]], grad_fn=<ViewBackward0>)\n",
      "context_vecs.shape: torch.Size([2, 6, 2])\n"
     ]
    }
   ],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, d_in, d_out, context_length, dropout, num_heads, qkv_bias=False):\n",
    "        super().__init__()\n",
    "        assert (d_out % num_heads == 0), \\\n",
    "            \"d_out must be divisible by num_heads\"\n",
    "\n",
    "        self.d_out = d_out\n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim = d_out // num_heads # Reduce the projection dim to match desired output dim\n",
    "\n",
    "        self.W_query = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.W_key = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.W_value = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.out_proj = nn.Linear(d_out, d_out)  # Linear layer to combine head outputs\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.register_buffer(\n",
    "            \"mask\",\n",
    "            torch.triu(torch.ones(context_length, context_length),\n",
    "                       diagonal=1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        b, num_tokens, d_in = x.shape\n",
    "        # As in `CausalAttention`, for inputs where `num_tokens` exceeds `context_length`, \n",
    "        # this will result in errors in the mask creation further below. \n",
    "        # In practice, this is not a problem since the LLM (chapters 4-7) ensures that inputs  \n",
    "        # do not exceed `context_length` before reaching this forwar\n",
    "\n",
    "        keys = self.W_key(x) # Shape: (b, num_tokens, d_out)\n",
    "        queries = self.W_query(x)\n",
    "        values = self.W_value(x)\n",
    "\n",
    "        # We implicitly split the matrix by adding a `num_heads` dimension\n",
    "        # Unroll last dim: (b, num_tokens, d_out) -> (b, num_tokens, num_heads, head_dim)\n",
    "        keys = keys.view(b, num_tokens, self.num_heads, self.head_dim) \n",
    "        values = values.view(b, num_tokens, self.num_heads, self.head_dim)\n",
    "        queries = queries.view(b, num_tokens, self.num_heads, self.head_dim)\n",
    "\n",
    "        # Transpose: (b, num_tokens, num_heads, head_dim) -> (b, num_heads, num_tokens, head_dim)\n",
    "        keys = keys.transpose(1, 2)\n",
    "        queries = queries.transpose(1, 2)\n",
    "        values = values.transpose(1, 2)\n",
    "\n",
    "        # Compute scaled dot-product attention (aka self-attention) with a causal mask\n",
    "        attn_scores = queries @ keys.transpose(2, 3)  # Dot product for each head\n",
    "\n",
    "        # Original mask truncated to the number of tokens and converted to boolean\n",
    "        mask_bool = self.mask.bool()[:num_tokens, :num_tokens]\n",
    "\n",
    "        # Use the mask to fill attention scores\n",
    "        attn_scores.masked_fill_(mask_bool, -torch.inf)\n",
    "        \n",
    "        attn_weights = torch.softmax(attn_scores / keys.shape[-1]**0.5, dim=-1)\n",
    "        attn_weights = self.dropout(attn_weights)\n",
    "\n",
    "        # Shape: (b, num_tokens, num_heads, head_dim)\n",
    "        context_vec = (attn_weights @ values).transpose(1, 2) \n",
    "        \n",
    "        # Combine heads, where self.d_out = self.num_heads * self.head_dim\n",
    "        context_vec = context_vec.contiguous().view(b, num_tokens, self.d_out)\n",
    "        context_vec = self.out_proj(context_vec) # optional projection\n",
    "\n",
    "        return context_vec\n",
    "\n",
    "torch.manual_seed(123)\n",
    "\n",
    "batch_size, context_length, d_in = batch.shape\n",
    "d_out = 2\n",
    "mha = MultiHeadAttention(d_in, d_out, context_length, 0.0, num_heads=2)\n",
    "\n",
    "context_vecs = mha(batch)\n",
    "\n",
    "print(context_vecs)\n",
    "print(\"context_vecs.shape:\", context_vecs.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b52b4a9",
   "metadata": {},
   "source": [
    "4.1 Coding an LLM architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "bb00305e",
   "metadata": {},
   "outputs": [],
   "source": [
    "GPT_CONFIG_124M = {\n",
    "    \"vocab_size\": 50257,\n",
    "    \"context_length\": 1024,\n",
    "    \"emb_dim\": 768,\n",
    "    \"n_heads\": 12,\n",
    "    \"n_layers\": 12,\n",
    "    \"drop_rate\": 0.1,\n",
    "    \"qkv_bias\": False,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "4adf58e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class DummyGPTModel(nn.Module):\n",
    "    def __init__(self, cfg):\n",
    "        super().__init__()\n",
    "        self.tok_emb = nn.Embedding(cfg[\"vocab_size\"], cfg[\"emb_dim\"])\n",
    "        self.pos_emb = nn.Embedding(cfg[\"context_length\"], cfg[\"emb_dim\"])\n",
    "        self.drop_emb = nn.Dropout(cfg[\"drop_rate\"])\n",
    "        \n",
    "        # Use a placeholder for TransformerBlock\n",
    "        self.trf_blocks = nn.Sequential(\n",
    "            *[DummyTransformerBlock(cfg) for _ in range(cfg[\"n_layers\"])])\n",
    "        \n",
    "        # Use a placeholder for LayerNorm\n",
    "        self.final_norm = DummyLayerNorm(cfg[\"emb_dim\"])\n",
    "        self.out_head = nn.Linear(\n",
    "            cfg[\"emb_dim\"], cfg[\"vocab_size\"], bias=False\n",
    "        )\n",
    "\n",
    "    def forward(self, in_idx):\n",
    "        batch_size, seq_len = in_idx.shape\n",
    "        tok_embeds = self.tok_emb(in_idx)\n",
    "        pos_embeds = self.pos_emb(torch.arange(seq_len, device=in_idx.device))\n",
    "        x = tok_embeds + pos_embeds\n",
    "        x = self.drop_emb(x)\n",
    "        x = self.trf_blocks(x)\n",
    "        x = self.final_norm(x)\n",
    "        logits = self.out_head(x)\n",
    "        return logits\n",
    "\n",
    "\n",
    "class DummyTransformerBlock(nn.Module):\n",
    "    def __init__(self, cfg):\n",
    "        super().__init__()\n",
    "        # A simple placeholder\n",
    "\n",
    "    def forward(self, x):\n",
    "        # This block does nothing and just returns its input.\n",
    "        return x\n",
    "\n",
    "\n",
    "class DummyLayerNorm(nn.Module):\n",
    "    def __init__(self, normalized_shape, eps=1e-5):\n",
    "        super().__init__()\n",
    "        # The parameters here are just to mimic the LayerNorm interface.\n",
    "\n",
    "    def forward(self, x):\n",
    "        # This layer does nothing and just returns its input.\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "d9f03047",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[6109, 3626, 6100,  345],\n",
      "        [6109, 1110, 6622,  257]])\n"
     ]
    }
   ],
   "source": [
    "import tiktoken\n",
    "tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
    "batch = []\n",
    "txt1 = \"Every effort moves you\"\n",
    "txt2 = \"Every day holds a\"\n",
    "\n",
    "batch.append(torch.tensor(tokenizer.encode(txt1)))\n",
    "batch.append(torch.tensor(tokenizer.encode(txt2)))\n",
    "batch = torch.stack(batch, dim=0)\n",
    "print(batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "de51aae4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output Shape torch.Size([2, 4, 50257])\n",
      "tensor([[[-1.2034,  0.3201, -0.7130,  ..., -1.5548, -0.2390, -0.4667],\n",
      "         [-0.1192,  0.4539, -0.4432,  ...,  0.2392,  1.3469,  1.2430],\n",
      "         [ 0.5307,  1.6720, -0.4695,  ...,  1.1966,  0.0111,  0.5835],\n",
      "         [ 0.0139,  1.6754, -0.3388,  ...,  1.1586, -0.0435, -1.0400]],\n",
      "\n",
      "        [[-1.0908,  0.1798, -0.9484,  ..., -1.6047,  0.2439, -0.4530],\n",
      "         [-0.7860,  0.5581, -0.0610,  ...,  0.4835, -0.0077,  1.6621],\n",
      "         [ 0.3567,  1.2698, -0.6398,  ..., -0.0162, -0.1296,  0.3717],\n",
      "         [-0.2407, -0.7349, -0.5102,  ...,  2.0057, -0.3694,  0.1814]]],\n",
      "       grad_fn=<UnsafeViewBackward0>)\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(123)\n",
    "model = DummyGPTModel(GPT_CONFIG_124M)\n",
    "logits = model(batch)\n",
    "print(\"Output Shape\", logits.shape)\n",
    "print(logits)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "891dbf13",
   "metadata": {},
   "source": [
    "4.2 Normalizing activations with layer normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "7bb8e5f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.2260, 0.3470, 0.0000, 0.2216, 0.0000, 0.0000],\n",
      "        [0.2133, 0.2394, 0.0000, 0.5198, 0.3297, 0.0000]],\n",
      "       grad_fn=<ReluBackward0>)\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(123)\n",
    "batch_example = torch.randn(2, 5)\n",
    "layer = nn.Sequential(nn.Linear(5,6), nn.ReLU())\n",
    "out = layer(batch_example)\n",
    "print(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "id": "cb6d1b8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean: tensor([[0.1324],\n",
      "        [0.2170]], grad_fn=<MeanBackward1>)\n",
      "Variance: tensor([[0.0231],\n",
      "        [0.0398]], grad_fn=<VarBackward0>)\n"
     ]
    }
   ],
   "source": [
    "mean = out.mean(dim=-1, keepdim=True)\n",
    "var = out.var(dim=-1, keepdim=True)\n",
    "print(\"Mean:\", mean)\n",
    "print(\"Variance:\", var)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "id": "e3bbc944",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Normalized Output: tensor([[ 0.6159,  1.4126, -0.8719,  0.5872, -0.8719, -0.8719],\n",
      "        [-0.0189,  0.1121, -1.0876,  1.5173,  0.5647, -1.0876]],\n",
      "       grad_fn=<DivBackward0>)\n",
      "Mean after normalization: tensor([[    0.0000],\n",
      "        [    0.0000]], grad_fn=<MeanBackward1>)\n",
      "Variance after normalization: tensor([[1.0000],\n",
      "        [1.0000]], grad_fn=<VarBackward0>)\n"
     ]
    }
   ],
   "source": [
    "out_norm = (out - mean) / torch.sqrt(var)\n",
    "mean = out_norm.mean(dim=-1, keepdim=True)\n",
    "var = out_norm.var(dim=-1, keepdim=True)\n",
    "\n",
    "print(\"Normalized Output:\", out_norm)\n",
    "print(\"Mean after normalization:\", mean)\n",
    "print(\"Variance after normalization:\", var)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "id": "f8b3bf73",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean:\n",
      " tensor([[    0.0000],\n",
      "        [    0.0000]], grad_fn=<MeanBackward1>)\n",
      "Variance:\n",
      " tensor([[1.0000],\n",
      "        [1.0000]], grad_fn=<VarBackward0>)\n"
     ]
    }
   ],
   "source": [
    "torch.set_printoptions(sci_mode=False)\n",
    "print(\"Mean:\\n\", mean)\n",
    "print(\"Variance:\\n\", var)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "id": "3eddc4a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LayerNorm(nn.Module):\n",
    "    def __init__(self, emb_dim):\n",
    "        super().__init__()\n",
    "        self.eps = 1e-5\n",
    "        self.scale = nn.Parameter(torch.ones(emb_dim))\n",
    "        self.shift = nn.Parameter(torch.zeros(emb_dim))\n",
    "\n",
    "    def forward(self, x):\n",
    "        mean = x.mean(dim=-1, keepdim=True)\n",
    "        var = x.var(dim=-1, keepdim=True, unbiased=False)\n",
    "        norm_x = (x - mean) / torch.sqrt(var + self.eps)\n",
    "        return self.scale * norm_x + self.shift\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "id": "81ec753b",
   "metadata": {},
   "outputs": [],
   "source": [
    "ln = LayerNorm(emb_dim=5)\n",
    "out_ln = ln(batch_example)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "id": "1c625aa3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean:\n",
      " tensor([[    -0.0000],\n",
      "        [     0.0000]], grad_fn=<MeanBackward1>)\n",
      "Variance:\n",
      " tensor([[1.0000],\n",
      "        [1.0000]], grad_fn=<VarBackward0>)\n"
     ]
    }
   ],
   "source": [
    "mean = out_ln.mean(dim=-1, keepdim=True)\n",
    "var = out_ln.var(dim=-1, unbiased=False, keepdim=True)\n",
    "\n",
    "print(\"Mean:\\n\", mean)\n",
    "print(\"Variance:\\n\", var)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90d19b32",
   "metadata": {},
   "source": [
    "4.3 Implementing a feed forward network with GELU activations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "id": "42c8e421",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GELU(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    def forward(self, x):\n",
    "        return 0.5 * x * (1 + torch.tanh(\n",
    "            torch.sqrt(torch.tensor(2.0 / torch.pi)) * \n",
    "            (x + 0.044715 * torch.pow(x, 3))\n",
    "        ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "id": "bfbcc02d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 800x300 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "gelu, relu = GELU(), nn.ReLU()\n",
    "\n",
    "# Some sample data\n",
    "x = torch.linspace(-3, 3, 100)\n",
    "y_gelu, y_relu = gelu(x), relu(x)\n",
    "\n",
    "plt.figure(figsize=(8, 3))\n",
    "for i, (y, label) in enumerate(zip([y_gelu, y_relu], [\"GELU\", \"ReLU\"]), 1):\n",
    "    plt.subplot(1, 2, i)\n",
    "    plt.plot(x, y)\n",
    "    plt.title(f\"{label} activation function\")\n",
    "    plt.xlabel(\"x\")\n",
    "    plt.ylabel(f\"{label}(x)\")\n",
    "    plt.grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "id": "ae8c586d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, cfg):\n",
    "        super().__init__()\n",
    "        self.layers = nn.Sequential(\n",
    "            nn.Linear(cfg[\"emb_dim\"], 4 * cfg[\"emb_dim\"]),  # First linear layer\n",
    "            GELU(),\n",
    "            nn.Linear(4 * cfg[\"emb_dim\"], cfg[\"emb_dim\"]),  # Second linear layer\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.layers(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "id": "a70ddb5f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output shape: torch.Size([2, 3, 768])\n",
      "Output: tensor([[[ 0.1238,  0.0457,  0.0939,  ...,  0.1107,  0.0167, -0.1992],\n",
      "         [ 0.1574, -0.0282,  0.0049,  ...,  0.0026,  0.1120, -0.1075],\n",
      "         [ 0.1184, -0.0052,  0.0839,  ...,  0.1662,  0.0112, -0.1685]],\n",
      "\n",
      "        [[ 0.1302,  0.0630,  0.1050,  ...,  0.1439,  0.0562, -0.1128],\n",
      "         [ 0.1249, -0.0073,  0.1022,  ...,  0.0417,  0.0381, -0.0828],\n",
      "         [ 0.0494,  0.0654,  0.0347,  ...,  0.0701,  0.0793, -0.1810]]],\n",
      "       grad_fn=<ViewBackward0>)\n"
     ]
    }
   ],
   "source": [
    "ffn = FeedForward(GPT_CONFIG_124M)\n",
    "x = torch.rand(2,3,768)\n",
    "out = ffn(x)\n",
    "print(\"Output shape:\", out.shape)\n",
    "print(\"Output:\", out)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96c21fbf",
   "metadata": {},
   "source": [
    "4.4 Adding shortcut connections\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "id": "bc58fd91",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ExampleDeepNeuralNetwork(nn.Module):\n",
    "    def __init__(self, layer_sizes, use_shortcut):\n",
    "        super().__init__()\n",
    "        self.use_shortcut = use_shortcut\n",
    "        self.layers = nn.ModuleList([\n",
    "            nn.Sequential(nn.Linear(layer_sizes[0], layer_sizes[1]), GELU()),\n",
    "            nn.Sequential(nn.Linear(layer_sizes[1], layer_sizes[2]), GELU()),\n",
    "            nn.Sequential(nn.Linear(layer_sizes[2], layer_sizes[3]), GELU()),\n",
    "            nn.Sequential(nn.Linear(layer_sizes[3], layer_sizes[4]), GELU()),\n",
    "            nn.Sequential(nn.Linear(layer_sizes[4], layer_sizes[5]), GELU())\n",
    "        ])\n",
    "\n",
    "    def forward(self, x):\n",
    "        for layer in self.layers:\n",
    "            # Compute the output of the current layer\n",
    "            layer_output = layer(x)\n",
    "            # Check if shortcut can be applied\n",
    "            if self.use_shortcut and x.shape == layer_output.shape:\n",
    "                x = x + layer_output\n",
    "            else:\n",
    "                x = layer_output\n",
    "        return x\n",
    "\n",
    "def print_gradients(model, x):\n",
    "    # Forward pass\n",
    "    output = model(x)\n",
    "    target = torch.tensor([[0.]])\n",
    "\n",
    "    # Calculate loss based on how close the target\n",
    "    # and output are\n",
    "    loss = nn.MSELoss()\n",
    "    loss = loss(output, target)\n",
    "    \n",
    "    # Backward pass to calculate the gradients\n",
    "    loss.backward()\n",
    "\n",
    "    for name, param in model.named_parameters():\n",
    "        if 'weight' in name:\n",
    "            # Print the mean absolute gradient of the weights\n",
    "            print(f\"{name} has gradient mean of {param.grad.abs().mean().item()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "id": "613f4efb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "layers.0.0.weight has gradient mean of 0.00020173587836325169\n",
      "layers.1.0.weight has gradient mean of 0.0001201116101583466\n",
      "layers.2.0.weight has gradient mean of 0.0007152041653171182\n",
      "layers.3.0.weight has gradient mean of 0.001398873864673078\n",
      "layers.4.0.weight has gradient mean of 0.005049646366387606\n"
     ]
    }
   ],
   "source": [
    "layer_sizes = [3, 3, 3, 3, 3, 1]  \n",
    "\n",
    "sample_input = torch.tensor([[1., 0., -1.]])\n",
    "\n",
    "torch.manual_seed(123)\n",
    "model_without_shortcut = ExampleDeepNeuralNetwork(\n",
    "    layer_sizes, use_shortcut=False\n",
    ")\n",
    "print_gradients(model_without_shortcut, sample_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "id": "e2b0c62e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "layers.0.0.weight has gradient mean of 0.22169792652130127\n",
      "layers.1.0.weight has gradient mean of 0.20694106817245483\n",
      "layers.2.0.weight has gradient mean of 0.32896995544433594\n",
      "layers.3.0.weight has gradient mean of 0.2665732502937317\n",
      "layers.4.0.weight has gradient mean of 1.3258541822433472\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(123)\n",
    "model_with_shortcut = ExampleDeepNeuralNetwork(\n",
    "    layer_sizes, use_shortcut=True\n",
    ")\n",
    "print_gradients(model_with_shortcut, sample_input)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
